---
title: 'Unit 3 Walkthrough: School Leader Network Mechanism'
subtitle: "ECI 589 Social Network Analysis and Education"
author: "Dr. Shaun Kellogg"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. PREPARE

For the Unit 3 Walkthrough: School Leader Selection Mechanisms we will
once again visit research conducted by Alan Daly and colleagues as we
attempt to replicate some of the analyses described in Chapter 9:
Network Data and Statistical Models from Social Network Analysis and
Education [@carolan2014]. In this walkthrough we'll revisit and and then
move beyond the visual and mathematical descriptions of networks
explored so far to uncover generative processes, or mechanisms, that
attempt to explain how school leaders select peers for collaboration or
confidential exchanges.

More specifically, this walkthrough will cover the following topics
pertaining to each data-intensive workflow process:

1.  **Prepare**: Prior to analysis, we'll take a look at the context
    from which our data came, formulate some research questions, and get
    introduced the {rtweet} R package for using the Twitter API.

2.  **Wrangle**: Wrangling data entails the work of manipulating,
    cleaning, transforming, and merging data. In section 2 we will learn
    about the {tidygraph} package for creating network objects.

3.  **Explore**: In section 3, we use the {tidygraph} package and the
    companion {ggraph} package to calculate a range of centrality
    measures and learn how to illustrate some of these stats through
    network visualization.

4.  **Model**: We wrap up our analysis in Section 4 by introducing
    community detection and sentiment analysis algorithms for
    identifying groups and gauging sentiment about the common core.

5.  **Communicate**: We briefly reflect on our walkthrough in
    preparation for our independent analysis next week.

## 1a. Review the Research

Recall from [Social Network Analysis and Education: Theory, Methods &
Applications](https://methods.sagepub.com/book/social-network-analysis-and-education)
that @carolan2014 made the following distinctions between mathematical
and statistical approaches to social network analysis:

1.  **Mathematical approaches** focus on what network of actors "looks
    like" by describing the network using sociograms and/or network
    measures such as reciprocity, centrality, and density. However,
    these approaches "tend to regard the measured relationships and
    their strengths as accurately reflecting the real, final, or
    equilibrium status of the network."

2.  **Statistical approaches**, or statistical inference, on the other
    hand, focuses on "assessing the reproducibility or likelihood of an
    observed pattern" with the goal of explaining and ultimately
    predicting network structures and outcomes.

The [\#COMMONCORE Project](https://www.hashtagcommoncore.com) that we'll
examine next is an exemplary illustration of these four defining
features of the social network perspective.

### Daly's School Leaders Dataset

Leadership network data were collected at two school districts over 3
consecutive years.

#### Research Questions:

1.  Is there a relationship between the frequency of collaboration
    between school leaders and how often they turn to each other to
    discuss issues of a confidential nature?
2.  Do school leaders prefer to collaborate with those with whom they
    have collaborated in the past, or is there some other reason?
3.  Does gender or some other individual attribute predicts confidential
    exchanges between school leaders, or does some previous relation
    have a stronger effect?
4.  Does collaboration between leaders explain one's level of trust in
    one's administrative colleagues?
5.  Can we distinguish among different groups of school leaders based on
    how frequently they collaborate, and if so, are these groupings
    related to the level at which they work (school versus district)?

#### Data Collection

For each consecutive year, school district leaders were invited to
complete a survey that collected individual:

-   **Demographic information** (e.g., gender, ethnicity, marital
    status, age, years of experiences),

-   **Network relationships types** (e.g., collaboration, confidential,
    energy, expertise, support you approach, support approach you,
    work-related issues, input, recognition, best practices, and
    innovation), efficacy, and trusting relationships.

-   **Frequency of interactions** they have with those nominated
    individuals on a four-point frequency scale ranging from 1 (the
    least frequent) to 4 (1--2 times a week).

-   **Leadership efficacy** items were designed based on the Principal
    Efficacy Scale used in Daly et al. (2011) and Tschannen-Moran and
    Gareis's (2004) studies. The efficacy scale includes 18 items rated
    on a 9-point Likert scale ranging from 1 (None at all) to 9 (A great
    deal).

-   **Trust** scale contains eight items rated on a 7-point Likert scale
    ranging from 1 (Strongly disagree) to 7 (Strongly agree) modified
    from Tschannen-Moran and Hoy (2003)

#### Analyses

Assumptions of independence Simulations...

-   **QAP/MR**: The quadratic assignment procedure (QAP) developed by
    Hubert (1987) and Krackhardt (1987b) tests the null hypothesis of no
    correlation between the two networks and adjusts for this dependence
    between networks by repeatedly permuting the order of rows and
    columns of one of the networks while keeping the other network
    intact. The QAP is based on regression models and permutation tests
    for valued (i.e., continuous) relational variables.

-   ERGM:

#### Key Findings

Findings here:

1.  **Is there a relationship between the frequency of collaboration
    between school leaders and how often they turn to each other to
    discuss issues of a confidential nature?** While collaboration in
    year 1 does not significantly predict collaboration in year 3,
    confidential exchanges in year 1 does, which suggests that
    collaboration among school leaders provides an important foundation
    for more sensitive, perhaps even deeper, relations (e.g.,
    confidential exchanges) at a later point in time
2.  

### **ðŸ‘‰ Your Turn** **â¤µ**

For our Unit 2 Walkthrough, we'll apply some of the same techniques used
by this study including some quick attempts at applying community
detection algorithms and automated text analysis using a few new
packages in R.

You are likely already familiar with some of the techniques and
approaches used in this study, however. For example, take a quick look
at the [*Explore the Networks*](https://www.hashtagcommoncore.com/#2-1)
section from Act 2: Central Actors and the Transmitters, Transceivers
and Transcenders identified by their analysis.

In the space below, type a brief response to the following questions:

1.  What network measures introduced in Unit 1 do think were used to
    identify these three types of "central actors"?

    -   YOUR RESPONSE HERE

Now check your response by looking at the [methods
page](https://www.hashtagcommoncore.com/project/methodology) of the
\#COMMONCORE Project.

1.  Were you correct?

    -   YOUR RESPONSE HERE

## 1b. Identify a Question(s)

Recall from above that our focus for this walkthrough is research
question 2:

> How are social media-enabled social networks changing the discourse in
> American politics that produces and sustains social policy?

For Unit 2, we are going to focus our questions on something a bit less
ambitious but inspired by this work:

1.  Who are the transmitters, transceivers, and transcenders in our
    Common Core Twitter network?
2.  What subgroups, or factions, exist in our network?
3.  Which actors in our network tend to be more opposed to the Common
    Core?

To address the latter question, we'll introduce a common text mining
technique in the Model section for gauging sentiment of social media
posts.

### **ðŸ‘‰ Your Turn** **â¤µ**

Based on what you know about networks and the context so far, what other
research questions might ask we ask in this context that a social
network perspective might be able to answer?

In the space below, type a brief response to the following questions:

-   YOUR RESPONSE HERE

## **1c. Set Up Project**

As highlighted inÂ [Chapter 6 of Data Science in Education Using
R](https://datascienceineducation.com/c06.html)Â (DSIEUR), one of the
first steps of every workflow should be to set up your "Project" within
RStudio. Recall that:

> A **Project** is the home for all of the files, images, reports, and
> code that are used in any given project

Since we are working in RStudio Cloud, a Project has already been set up
for you as indicated by the `unit-3.Rproj` file in your main directory
in the Files pane.

### Load Libraries

Recall that **packages**, or libraries, are shareable collections of R
code that can contain functions, data, and/or documentation and extend
the functionality of R. You can always check to see which packages have
already been installed and loaded into RStudio Cloud by looking at the
the Files, Plots, & Packages Pane in the lower right hand corner.

Let's go ahead and load the packages from Unit 1 since we'll be using
them again very soon:

```{r}
library(tidyverse)
library(tidygraph)
library(ggraph)
```

#### statnet ðŸ“¦

![](img/statnetlogo.png){width="20%"}

Similar to the collection of packages contained in the {tidyverse}
package, the Statnet Team [@handcock:statnet] has developed a suite of R
packages for the management, exploration, statistical analysis,
simulation and vizualization of network data. The statistical modeling
framework used in {statnet} relies on Exponential-family Random Graph
Models (ERGMs).

Let's load the {statnet} package that we'll be using in just a bit to
accomplish all three of the goals listed above:

```{r}
library(statnet)
```

That's it! You're ready ready to start wrangling some tweets!!!

------------------------------------------------------------------------

# 2. WRANGLE

In general, data wrangling involves some combination of cleaning,
reshaping, transforming, and merging data [@wickham2016r]. The
importance of data wrangling is difficult to overstate, as it involves
the initial steps of going from the raw data to a dataset that can be
explored and modeled [@krumm2018].

For our data wrangling this week, we're keeping it simple since working
with network data is a bit of a departure from our working with
rectangular data frames. Our primary goals for Unit 1 are learning how
to:

a.  **Import Tweets**. In this section, we introduce
    theÂ `rtweet`Â package and some key functions to search for tweets or
    users of interest.

b.  **Create a Network Object**. Before performing network analyses,
    we'll need to convert our data frames into special data format for
    working with relational data.

c.  **Simplify Network**. Finally, we'll learn about a handy
    `simplify()` function in the {igraph} package for collapsing
    multiple ties between actors and removing "self-loops."

## 2a. Import Tweets

The Import Tweets section introduces the following functions from the
`rtweet` package for reading Twitter data into R:

-   [`search_tweets()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets)
    Pulls up to 18,000 tweets from the last 6-9 days matching provided
    search terms.
-   [`search_tweets2()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets)
    Returns data from **multiple** search queries.
-   [`get_timelines()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets)
    Returns up to 3,200 tweets of one or more specified Twitter users.

### Search Tweets

Since one of our goals for this walkthrough is a very crude replication
of the study by @supovitz2017commoncore, let's begin by introducing the
`search_tweets()` function to try reading into R 5,000 tweets containing
the commoncore hashtag and store as a new data frame `ngss_all_tweets`.

Type or copy the following code into your R script or console and run:

```{r}
ccss_tweets <- search_tweets(q = "#commoncore", n=5000)
```

You likely saw in the output or console a download message like the one
below that stopped well short of 100%. That is pretty typical and just
indicates the API returned fewer tweets than requested.

Note also that the first argument `q =` that the `search_tweets()`
function expects is the search term included in quotation marks and that
`n =` specifies the maximum number of tweets

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the code chunk below to view your new `ccss_tweets` data frame using
one of the [various view
methods](https://cmdlinetips.com/2020/06/get-a-peek-at-dataframe-in-r/)
for looking at your data in R:

```{r}
#YOUR CODE HERE
```

1.  How many tweets did our query using the Twitter API actually return?
    How many variables?

    -   YOUR RESPONSE HERE

2.  Why do you think our query pulled in far less than 5,000 tweets
    requested? **Hint:** Type `?search_tweets` in your console to see
    the support documentation for this function.

    -   YOUR RESPONSE HERE

3.  Does our query also include retweets? How do you know?

    -   YOUR RESPONSE HERE

### Remove Retweets

While the authors explicitly mentioned including retweets when
constructing their network, `search_tweets()` function has an argument
that includes retweets by default, but can be changed to exclude
retweets if desired.

Even though we will want to include retweets to construct our final
network, let's test out the `include_rts =` argument and set it to
`FALSE` to remove any retweets:

```{r}
ccss_tweets_1 <- search_tweets("#commoncore", 
                                   n=500, 
                                   include_rts = FALSE)
```

Take a look at your environment pane and see how many tweets were
excluded by adding this argument.

### Using the OR Operator

If you recall from the Prepare section, the authors accessed tweets and
user information from using multiple search terms, including:
*commoncore*, *ccss* and *stopcommoncore.*

Let's modify our query using the `OR` operator to also include "ccss"
and "stopcommoncore" so it will return tweets containing either any of
these three terms. And since we've included additional search terms
let's also increase the number of requested tweets to 5,000.

```{r}
ccss_tweets_2 <- search_tweets(q = "#commoncore OR #stopcommoncore OR #ccss",
                               n=5000)
```

Wow, those additional search terms dramatically increased the number of
tweets downloaded!

### **ðŸ‘‰ Your Turn** **â¤µ**

Try including both search terms but excluding the `OR` operator to
answer the following question:

1.  Does excluding the `OR` operator return more tweets, the same number
    of tweets, or fewer tweets? Why?

    -   YOUR RESPONSE HERE

2.  What other useful arguments does the `search_tweet()` function
    contain? Try adding one and see what happens.

    -   YOUR RESPONSE HERE

**Hint:** Use the `?search_tweets` help function to learn more about the
`q` argument and other arguments for composing search queries.

### Filter for English Tweets

If you haven't already done so, take a look at the tweets in our
`ccss_tweets_2` data frame. You probably noticed that many of these
tweets were in Spanish, as indicated by the "es" in the `lang` column of
our data frame.

Let's take a quick count of the number of tweets in each language of our
data frame by sending our `ccss_tweets_2` data frame to the `count()`
function using the `%>%` operator, specifying `lang` as the column whose
values we want to count and sorting from high to low:

```{r}
ccss_tweets_2 %>%
  count(lang, sort = TRUE)
```

Surprisingly, the majority of these tweets are in Spanish. For the
purpose of both limiting the size of our network for this analysis and
also because we are interested in doing some quick sentiment analysis
towards the end of our walkthrough, let's limit our tweets to just those
in the English language.

To download only English tweets, we will need include the `lang =`
argument and set it to "en":

```{r}
ccss_tweets_3 <- search_tweets(q = "#commoncore OR #stopcommoncore OR #ccss",
                               n=5000,
                               lang = "en")
```

#### Save Tweets to File

Finally, let's save our tweet files to use in later exercises since
tweets have a tendency to change every minute.

```{r, eval=FALSE}
write_csv(ccss_tweets_3, "data/ccss-tweets.csv")
```

Uh oh! We received an error because data stored in the `hashtags` column
is not stored as typical character, number, logical, or date data type,
but rather it is stored as a "list."

Use the `glimpse()` function to see which other columns store data as
lists:

```{r}
glimpse(ccss_tweets_3)
```

Since columns containing lists can not be stored in flat files like
.csv, we'll need to "flatten" our data in these columns using the aptly
named `flatten()` function from the {rtweets} package:

```{r}
ccss_tweets_4 <- rtweet::flatten(ccss_tweets_3)
```

Note that we included the package name followed by a double colon
`rtweet::` when calling our function to avoid confusion with other
functions using the same name.

Now we can use the `read_csv()` function to save our data frame to our
folder.

```{r}
write_csv(ccss_tweets_4, "data/ccss-tweets.csv")
```

Check your data folder to make sure this file has been save. You will
also see a `ccss-tweets-5.csv` file which I have save earlier and which
we will use in the next section to ensure we all have the same results.

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the `search_tweets` function to create your own custom query for a
twitter hashtag or topic(s) of interest and save as a .csv file in the
data folder.

```{r}
#YOUR CODE HERE
```

**Note:** You may want to use this data for your independent analysis
next week.

#### Other Useful Queries

For your independent analysis, you may be interest in exploring posts by
specific users rather than topics, key words, or hashtags. Yes, there is
a function for that too!

For example, let's create another list containing the usernames of me
and some of my colleagues at the Friday Institute using the `c()`
function again and use the `get_timelines()` function to get the most
recent tweets from each of those users:

```{r}
fi <- c("sbkellogg", "toosweetgeek", "haspires", "tarheel93", "drcallie_tweets", "AlexDreier")

fi_tweets <- fi %>%
  get_timelines(include_rts=FALSE)
```

And let's use the `sample_n()` function from the `dplyr` package to pick
10 random tweets and use `select()` to select and view just the
`screenname` and `text` columns that contains the user and the content
of their post:

```{r}
sample_n(fi_tweets, 10) %>%
  select(screen_name, text)
```

We've only scratched the surface of the number of functions available in
the `rtweets` package for searching Twitter. Use the following function
to take a look at other ways you can query data using the Twitter API:

```{r, eval=F}
vignette("intro", package="rtweet")
```

### **ðŸ‘‰ Your Turn** **â¤µ**

To conclude Section 2a, try one of the following search functions from
the `rtweet` vignette:

1.  `get_timelines()` Get the most recent 3,200 tweets from users.
2.  `stream_tweets()` Randomly sample (approximately 1%) from the live
    stream of all tweets.
3.  `get_friends()` Retrieve a list of all the accounts a user follows.
4.  `get_followers()` Retrieve a list of the accounts following a user.
5.  `get_favorites()` Get the most recently favorited statuses by a
    user.
6.  `get_trends()` Discover what's currently trending in a city.
7.  `search_users()` Search for 1,000 users with the specific hashtag in
    their profile bios.

```{r}
#YOUR CODE HERE
```

## 2b. Create Edgelist from Tweets

Recall from Unit 1 that an edge-list that contains information about
each tie, or relation between two actors in a network. In our previous
walkthrough, a "tie" is a reply by one participant in the discussion
forum to the post of another participant -- or in some cases to their
own post. For our analysis of tweets, we'll use the same approach used
by @supovitz2017commoncore. Specifically, each node is an individual
Twitter user (person, group, institution, etc.) and the connection
between each node is the tweet, retweet, or mention/reply.

### Import Saved Tweets

Before we proceed, let's first use the `read_csv()` function from the
{readr} package introduced in the Getting Started walkthrough to read
the `ccss-tweets-5.csv` file saved in or data folder so we are all
working with the same data:

```{r}
ccss_tweets_5 <- read_csv("data/ccss-tweets-5.csv")
```

### **ðŸ‘‰ Your Turn** **â¤µ**

As highlighted in @estrellado2020e, cleaning and tidying network data
can be even more challenging than for other data sources, network data
often includes variables about both individuals and their relationships.
This is especially true of our data downloaded using the Twitter API.

Take a look at the data we just imported using one of your favorite
methods for inspecting data and in the space below, identify the columns
you think could be used to construct an edge list.

-   YOUR RESPONSE HERE

### Extract Edges and Nodes

If one of the columns you indicated in your response above included
`screen_name` nice work! You may also have noticed that the
`mentions_screen_name` column also includes the names of those in the
reply column.

#### Create Edgelist

Recall from Unit 1 that the first two columns in an edgelist should
include the nodes that make up a tie or dyad. Since the only two columns
we need to construct our edgelist is the `screen_name` of the tweet
author and the screen names of those included in the mentions, let's
`relocate()` and rename those columns and then `select()` those columns
along with any attributes that we think might be useful for analysis
later on, like the timestamp or content of the tweet, i.e. `text`.

```{r}
ties_1 <-  ccss_tweets_5 %>%
  relocate(sender = screen_name, # rename scree_name to sender
           target = mentions_screen_name) %>% # rename to receiver
  select(sender,
         target,
         created_at,
         text)
```

Our edgelist with attributes is almost ready, but we have a couple
issues we still need to deal with.

As you may have noticed, our receiver column contains the names of
multiple Twitter users, but a dyad or tie can only be between two actors
or nodes.

#### "Unnest" User Names

In order to place each target user in a separate row that corresponds
with each sender of the tweet, we will need to "unnest" these names
using a package that those who took the ECI 588 Text Mining course will
be very familiar with.

#### tidytext ðŸ“¦

![](img/tidytext.png){width="20%"}

The {tidytext} package helps to convert text into data frames of
individual words, making it easy to to manipulate, summarize, and
visualize text using using familiar functions form the {tidyverse}
collection of packages.

Let's go ahead and load the {tidytext} package:

```{r}
library(tidytext)
```

For a more comprehensive introduction to the `tidytext` package, we
cannot recommend enough the free online book, *Text Mining with R: A
Tidy Approach* [@silge2017text]. If you're interested in pursuing text
analysis beyond this walkthrough, this will be a go to reference.

Let's go ahead and apply the \`un

```{r}
ties_2 <- ties_1 %>%
  unnest_tokens(input = target,
                output = receiver,
                to_lower = FALSE) %>%
  relocate(sender, receiver)
```

There is A LOT to unpack with this function. First notice
thatÂ `unnest_tokens`Â expects a data frame as the first argument, which
we supplied using teh %\>% operator, followed by two column names. The
first is an input column that the text comes from, `target` in this
case, which we want to "unnest" into separate rows. Also notice:

-   Other columns, such asÂ `sender`Â andÂ `text`, are retained.

-   By default, words are changed to lowercase, which makes them easier
    to compare or combine with other datasets. However, we can set
    theÂ `to_lower = FALSE`Â argument to turn off this behavior since
    capitalization is important for our user names.

Finally, we'll introduce the `drop_na()` function to remove the rows
with missing values from our `receiver` column since they are incomplete
dyads. We'll include these actors in our network later as part of our
nodelist in just a bit.

Let's drop our missing values and save as `ties` for our final data
frame

```{r}
ties <- ties_2 %>%
  drop_na(receiver)
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a quick look at our final edgelist and answer the following
question:

How many edges our in our CCSS network?

-   YOUR RESPONSE HERE

Using the tweets you downloaded with your own custom query from above,
create an edgelist with any desired edge attributes.

```{r}
# YOUR CODE HERE
```

### Node Attributes

The second file we need to create is a data frame that contains all the
nodes or actors in our network.

Regardless, let's read in our node attribute file and take a look at the
`actors` and their attributes included in our dataset:

```{r}
actors_1 <- ties_2 %>%
  pivot_longer(cols = sender:receiver, 
               names_to = "nodes",
               values_to = "screen_name")
```

Since

```{r}
actors <- actors_1 %>%
  select(screen_name) %>%
  distinct() %>%
  drop_na()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a quick look at our final `actors` data frame and answer the
following question:

How many nodes our in our CCSS network?

-   YOUR RESPONSE HERE

Using the tweets you downloaded with your own custom query from above,
create an nodelist containing all the unique actors in your network.

```{r}
# YOUR CODE HERE
```

## 2c. Create Network Object

In Unit 1 we learned about the {igraph} package for preparing network
objects for analysis. In this section we introduce a new package that
builds upon the "the well-oiled machinery of igraph" but allows us to
use some of the familiar functions and syntax from other {tidyverse}
packages. In this walkthrough we won't recreate all the processes like
simplifying graphs and adding edge weights, but will demonstrate some
features similar to and in addition to those found in the {igraph}
package.

### tidygraph ðŸ“¦

![](img/tidygraph.png){width="20%"}

The {[tidygraph](https://tidygraph.data-imaginist.com)} package is a
huge package that exports 280 different functions and methods. It more
or less wraps the full functionality ofÂ `igraph`Â in a tidy API giving
you access to almost all of theÂ `dplyr`Â verbs plus a few more, developed
for use with relational data. While network data itself is not tidy, it
can be envisioned as two tidy tables, one for node data and one for edge
data. The {tidygraph} package provides a way to switch between the two
tables andÂ `dplyr`Â verbs to manipulate them. Furthermore it provides
access to a lot of graph algorithms with return values that facilitate
their use in a tidy workflow.

Let's go ahead and load the {tidygraph} library:

```{r}
library(tidygraph)
```

### Combine Edges & Nodes

Before we can begin using many of the functions from the {tidygraph}
package for preparing and summarizing our Twitter network, we first need
to convert the data frames that we imported into a network object,
similar to what we did in Unit 1.

To do that, we will use the `tbl_graph()` function and include the
following arguments:

-   `edges =` expects a data frame, in our case `ties`, containing
    information about the edges in the graph. The nodes of each edge
    must either be in a `to` and `from` column, or in the two first
    columns like the data frame we provided.

-   `nodes =` expects a data frame, in our case `actors`, containing
    information about the nodes in the graph. If `to` and/or `from` are
    characters or names, like in our data frames, then they will be
    matched to the column named according to `node_key` in nodes, if it
    exists, or matched to the first column in the node list.

-   `directed =` specifies whether the constructed graph be directed and
    defaults to `TRUE` so we did not included it since our network is
    directed.

Let's go ahead and create our network graph, name it `network` and print
the output:

```{r}
network <- tbl_graph(edges = ties, 
                     nodes = actors)

network
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a look at the output for our simple graph now and answer the
following questions:

1.  Are the numbers and names of nodes and actors consistent with our
    `actors` and `ties` data frames? What about the integers included in
    the `from` and `to` columns of the Edge Data?

    -   YOUR RESPONSE HERE

2.  What do you think "components" refers to? **Hint:** see Chapter 6 of
    [@carolan2014].

    -   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of data wrangling section and are ready
to start analysis! Before proceeding further, knit your document and
check to see if you encounter any errors.

------------------------------------------------------------------------

# 3. EXPLORE

As noted in the Getting Started Walkthrough and experienced in Unit 1,
exploratory data analysis involves the processes of describing your data
(such as by calculating the means and standard deviations of numeric
variables, or counting the frequency of categorical variables) and,
often, visualizing your data prior to modeling.

In Section 3, we use the {tidygraph} package for retrieving network
descriptives and introduce the {ggraph} package to create a network
visualization to help illustrate these metrics. Specifically, in this
section we'll learn to:

a.  **Examine Basic Descriptives**. We focus primarily on actors and
    edges in this walkthrough, including the edges wights we added in
    the previous section as well as node degree, and import and fairly
    intuitive measure of centrality.

b.  **Make a Sociogram**. Finally, we wrap up the explore phases by
    learning to plot a network and tweak key elements like the size,
    shape, and position of nodes and edges to better at communicating
    key findings.

## 3a. Examine Basic Descriptives

As we noted in Unit 1, many analyses of social networks are primarily
descriptive and aim to either represent the network's underlying social
structure through data-reduction techniques or to characterize network
properties through network measures.

### Centrality

#### Node Degree

Recall from Unit 1 that:

> **Degree** is the number of ties to and from an ego. In a directed
> network, in-degree is the number of ties received, whereas out-degree
> is the number of ties sent.

The {tidygraph} package has an unique function called `activate()` that
allows us to treat the nodes in our network object as if they were a
standard data frame that we can then apply standard tidyverse functions
to like `select()`, `filter()`, and `mutate()`.

The latter function, `mutate()`, we can use to create new variables for
nodes such as measures of degree, in-degree, and out-degree using the
`centrality_degree()` function in the {tidygraph} package.

Run the following code to add degree measures to each of our nodes and
print the output:

```{r}
network_1 <- network %>%
  activate(nodes) %>%
  mutate(degree = centrality_degree(mode = "all")) %>%
  mutate(in_degree = centrality_degree(mode = "in")) %>%
  mutate(out_degree = centrality_degree(mode = "out"))

network_1
```

We now see that these simple measures of centrality have been added to
the nodes in our network.

We can also use the `activate()` function combined with the
`data.frame()` function to extract our new measures to a separate data
frame so we inspect our nodes individually and create some summary
statistics using the handy `summary()` function.

```{r}
node_measures <- network_1 %>% 
  activate(nodes) %>%
  data.frame()

summary(node_measures)
```

Despite a dramatic size difference from our network in Unit 1, we see
that typical nodes in this network also have relatively few connections,
though there are a few exceptions.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that one of our questions guiding this
analysis was:

> Who are the transmitters, transceivers, and transcenders in our Common
> Core Twitter network?

Use the code chunk below to inspect our `node_measures` data frame and
answer the questions above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

#### Other Centrality Measures

In Chapter 7 of @carolan2014, noted that degree centrality does not take
into account indirect ties among all the alters in an ego's network.Â We
were also introduced to a few other measures of centrality commonly used
in network analysis and applied to educational contexts:

-   **Closeness** includes data about the relation between each pair of
    ego's named alters and is intuitively appealing in that being
    "close" to others and may indicate how quickly an actor can exchange
    something with others or be the first to receive information.

-   **Betweenness** captures how actors control or mediate the relations
    between pairs of actors that are not directly connected and is an
    important indicator of control over information exchange or resource
    flows within a network.

Carolyn also notes that in addition to these three common centrality
measures, many others have been developed and can be calculated in most
common social network-analysis software applications.Â In fact,
{tidygraph} includes many of these measures and includes various
`centrality_` functions for calculating node and edge centrality. Type
`?centrality` in your console below and hit enter to view them all.

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the code chunk below to add these closeness and betweenness measures
to our `network_1` data frame and save as `network_2`. I've included
some basic code to get your started.

```{r}
network_2 <- network_1
  #YOUR CODE HERE
  
```

## 3b. Make a Sociogram

If you recall from Unit 1, network visualization can be used for a
variety of purposes, ranging from highlighting key actors to even
serving as works of art. These visual representations of the actors and
their relations, i.e. the network, are called a **sociogram**. Actors
who are most central to the network, such as those with higher node
degrees, are usually placed in the center of the sociogram and their
ties are placed near them. In this section, we'll briefly introduce the
{ggraph} package for creating attractive network visualizations.

### ggraph ðŸ“¦

![](img/ggraph.png){width="20%"}

Created by the same developer as {tidygraph},
{[ggraph](https://ggraph.data-imaginist.com/index.html)} -- pronounced
gg-raph or g-giraffe hence the logo -- is an extension of
{[ggplot](https://ggplot2.tidyverse.org)} aimed at supporting relational
data structures such as networks, graphs, and trees. Both packages are
more modern and widely adopted approaches data visualization in R.

While ggraph builds upon the foundation of ggplot and its API, it comes
with its own self-contained set of geoms, facets, etc., as well as
adding the concept ofÂ *layouts*Â to the [grammar of
graphics](https://ggplot2-book.org/introduction.html?q=grammar#what-is-the-grammar-of-graphics),
i.e. the "gg" in ggplot and ggraph.

Let's go ahead and load the {ggraph} library:

```{r}
library(ggraph)
```

### A Simple Sociogram

Very similar to how functions in the tidyverse use the `%>%` operator to
"pipe" functions together and progressively wrangle data, ggraph and
ggrplot use the `+` operator to "layer" functions together to
progressively build graphs.

Let's start with the first and simplest function `ggraph` and supply our
network graph:

```{r}
ggraph(network_1)
```

As you can see, this didn't produce much. All this function does is take
care of setting up the network object to plot along with creating the
layout for the plot based on the graph and the specified layout passed
in.

#### Add Layout

Let's go ahead and include the layout argument, which in addition to
including its own unique layouts, can incorporate layouts form {igraph}
like `fr`.

```{r}
ggraph(network_1, layout = "fr")
```

#### Add Nodes

Still nothing but that is because we haven't added the nodes yet. Let's
do that:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point() 
```

Well, at least we have our nodes now!

The "geom" in the `geom_non_point()` functions stands for "Geometric
elements", or geoms for short, and represent what you actually see in
the plot.

These geoms can include aesthetics, or aes for short, such as `alpha`
for transparency, as well as `colour`, `shape` and `size`.

Let's now add some "aesthetics" to our points by including the `aes()`
function and arguments such as `size =` which we can set to our
`in_degree` measures:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point(aes(size = in_degree, 
                    alpha = out_degree, 
                    colour = degree))
```

And let's add some node text and labels while were at it since this is
not a very large network. Since node labels are a geometric element, we
can apply aesthetics to them as well. Let's also include the `repel =`
argument that when set toÂ `TRUE`Â will avoid overlapping text.

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = in_degree/2,
                     alpha = degree),
                 repel=TRUE)
```

Much better! Even without the edges, using size and opacity has helped
to illustrate some of the "transmitters," "trancievers" and
"transcenders."

#### Add Edges

Now, let's connect the dots and add some edges that include some arrows
1mm in length as well as an end cap to keep them from overlapping the
nodes:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3)
```

#### Add a Theme

Finally, let's add a **theme,** which controls the finer points of
display, like the font size and background color. The `theme_graph()`
function add a theme specially tuned for graph visualizations. This
function removes redundant elements in order to put focus on the data
and if you type `?theme_graph` in the console you will get a sense of
the level of fine tuning you can do if desired.

Let's add `theme_graph()` to our sociogram and call it good for now:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Try modifying the code below by tweaking the included function/arguments
or adding new ones for
[layouts](https://ggraph.data-imaginist.com/articles/Layouts.html),
[nodes](https://ggraph.data-imaginist.com/articles/Nodes.html), and
[edges](https://ggraph.data-imaginist.com/articles/Edges.html) to make
our plot either more "aesthetically pleasing" or more purposeful in what
it's trying to communicate.

There are no right or wrong answers, just have some fun trying out
different approaches!

```{r}
ggraph(network_1, layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of the Explore section and are ready to
learn a little about network modeling! Before proceeding further, knit
your document and check to see if you encounter any errors.

# 4. MODEL

As highlighted inÂ [Chapter 3 of Data Science in Education Using
R](https://datascienceineducation.com/c03.html), theÂ **Model**Â step of
the data science process entails "using statistical models, from simple
to complex, to understand trends and patterns in the data."

## 4a. Identify Groups

In Chapter 6: Groups and Positions in Complete Networks of SNA and
Education [@carolan2014] we were introduced to both "bottom up" and "top
down" approaches for identifying groups in a network, as well as why
researchers may be interested in exploring these groups. He also notes
that:

> Unlike most social science, the idea is to identify these groups
> through their relational data, not an exogenous attribute such as
> grade level, departmental affiliation, or years of experience.Â 

In this section, we'll briefly explore a "top down" approach to
identifying these groups through the use of community detection
algorithms.

### Community Detection

Similar to the range of functions included for calculating node and edge
centrality, the {tidygraph} package includes various clustering
functions provided by the {igraph} package introduced in Unit 1.

Also similar to calculating centrality measures, we need to `activate()`
our nodes first before applying these community detection algorithms to
assign our nodes to groups.

Run the following code and take a print our new network graph to the
console to take a quick look

```{r}
network_3 <- network_2 %>%
  activate(nodes) %>%
  mutate(group = group_infomap())
  
network_3
```

**Note:** Some of these algorithms are designed for directed graphs,
while others are for undirected graphs.

Now that we've assigned our nodes to a group, let's modify our sociogram
from above to color our nodes by group assignment and remove the
`alpha =` argument to make them a little easier to see.

```{r}
network_3 %>%
  ggraph(layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      colour = group)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the second question guiding this
analysis was:

> What subgroups, or factions, exist in our network?

Use the code chunk below to extract the group assignment data frame and
answer the question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

## 4b. Identify Sentiment

Sentiment analysis (also known asÂ opinion mining) is a text mining
technique used to "systematically identify, extract, quantify, and study
affective states and subjective information." In this section, we'll
introduce and apply the {vader} package to gain some insight into the
"lexical tendencies" of our tweets.

#### The vader Package ðŸ“¦

![](img/vader.jpeg){width="20%"}\

The {vader} package is for the Valence Aware Dictionary for sEntiment
Reasoning (VADER), a rule-based model for general sentiment analysis of
social media text and specifically attuned to measuring sentiment in
microblog-like contexts.

To learn more about the {vader} package and its development, take a look
at the article by Hutto and Gilbert (2014), [VADER: A Parsimonious
Rule-based Model forSentiment Analysis of Social Media
Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).

Let's go ahead and load the vader library:

```{r}
library(vader)
```

The {vader} package basically has just one function, `vader_df()` that
does one thing and expects just one column from a data frame. Let's give
VADER the our `ccss_tweets_6` data frame that we created earlier and
include the `$` operator to select for analysis our `text` column
containing our tweets.

```{r}
summary_vader <- vader_df(ties$text)

summary_vader
```

Hutto, C. & Gilbert, E. (2014) provide an excellent summary of the VADER
package on their [GitHub repository](scores) and I've copied and
explanation of the scores below:

-   TheÂ `compound`Â score is computed by summing the valence scores of
    each word in the lexicon, adjusted according to the rules, and then
    normalized to be between -1 (most extreme negative) and +1 (most
    extreme positive). This is the most useful metric if you want a
    single unidimensional measure of sentiment for a given sentence.
    Calling it a 'normalized, weighted composite score' is accurate.

**NOTE:**Â TheÂ `compound`Â score is the one most commonly used for
sentiment analysis by most researchers, including the authors.

Let's use the `inner_join()` function to add these sentiment scores
values back to our `ties` data frame and take a quick look:

```{r}
tweet_sentiment <-inner_join(summary_vader, 
                             ties,
                             by = "text")

tweet_sentiment
```

Now that we have these joined, we can take a quick look at the sentiment
.

```{r}
user_sentiment <- tweet_sentiment %>%
  group_by(sender) %>%
  summarise(sentiment = mean(compound))

user_sentiment
```

Note that we have effectively just created some new node and edge
"attributes" that could be incorporated into our network visualization
to potentially help understand why groups may have formed as they did.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the final question guiding this
analysis was:

> Which actors in our network tend to be more opposed to the Common
> Core?

Use the code chunk below to inspect our data frame and answer the
question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

------------------------------------------------------------------------

# 5. COMMUNICATE

For your Independent Analysis assignment for Unit 2 next week, you'll
create either a simple report or slide deck using an R Markdown document
just like this to share out some key findings from your analysis.
Regardless of whether you plan to talk us through your analysis and
findings with a presentation or walk us through with a brief written
report, your presentation or report should address the following
questions:

1.  **Purpose**. What question or questions are guiding your analysis?
    What did you hope to learn by answering these questions and why
    should your audience care about your findings?

2.  **Methods**. What data did you selected for analysis? What steps did
    you take took to prepare your data for analysis and what techniques
    you used to analyze your data? These should be fairly explicit with
    your embedded code.

3.  **Findings**. What did you ultimately find? How do your "data
    products" help to illustrate these findings? What conclusions can
    you draw from your analysis?

4.  **Discussion**. What were some of the strengths and weaknesses of
    your analysis? How might your audience use this information? How
    might you revisit or improve upon this analysis in the future?

### **ðŸ‘‰ Your Turn** **â¤µ**

Now that you've become more familiar with this dataset and the social
network perspective, what other aspects of this dataset, or a dataset
you are interested in exploring, could you investigate?

-   YOUR RESPONSE HERE

What specific research questions might you ask that would be helpful for
being understanding and improving learning, or the context in which the
data is collected? How could other approaches like sentiment analysis

-   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You've finished the Unit 2 Guided Walkthrough and are ready
for some independent analysis next week!

To complete this assignment, knit your document and send me an email at
sbkellog\@ncsu.edu letting me know you're all set.
