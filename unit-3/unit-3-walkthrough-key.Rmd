---
title: 'Unit 3 Walkthrough: School Leader Network Mechanism'
subtitle: "ECI 589 Social Network Analysis and Education"
author: "Dr. Shaun Kellogg"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. PREPARE

For the Unit 3 Walkthrough: School Leader Selection Mechanisms we will
once again visit research conducted by Alan Daly and colleagues as we
attempt to replicate some of the analyses described in Chapter 9:
Network Data and Statistical Models from Social Network Analysis and
Education [@carolan2014]. In this walkthrough we'll revisit and and then
move beyond the visual and mathematical descriptions of networks
explored so far to uncover generative processes, or mechanisms, that
attempt to explain how school leaders select peers for collaboration or
confidential exchanges.

More specifically, this walkthrough will cover the following topics
pertaining to each data-intensive workflow process:

1.  **Prepare**: Prior to analysis, we'll take a look at the context
    from which our data came, formulate some research questions, and get
    introduced the {rtweet} R package for using the Twitter API.

2.  **Wrangle**: Wrangling data entails the work of manipulating,
    cleaning, transforming, and merging data. In section 2 we will learn
    about the {tidygraph} package for creating network objects.

3.  **Explore**: In section 3, we use the {tidygraph} package and the
    companion {ggraph} package to calculate a range of centrality
    measures and learn how to illustrate some of these stats through
    network visualization.

4.  **Model**: We wrap up our analysis in Section 4 by introducing
    community detection and sentiment analysis algorithms for
    identifying groups and gauging sentiment about the common core.

5.  **Communicate**: We briefly reflect on our walkthrough in
    preparation for our independent analysis next week.

## 1a. Review the Research

Recall from [Social Network Analysis and Education: Theory, Methods &
Applications](https://methods.sagepub.com/book/social-network-analysis-and-education)
that @carolan2014 made the following distinctions between mathematical
and statistical approaches to social network analysis:

1.  **Mathematical approaches** focus on what network of actors "looks
    like" by describing the network using sociograms and/or network
    measures such as reciprocity, centrality, and density. However,
    these approaches "tend to regard the measured relationships and
    their strengths as accurately reflecting the real, final, or
    equilibrium status of the network."

2.  **Statistical approaches**, or statistical inference, on the other
    hand, focuses on "assessing the reproducibility or likelihood of an
    observed pattern" with the goal of explaining and ultimately
    predicting network structures and outcomes.

The [\#COMMONCORE Project](https://www.hashtagcommoncore.com) that we'll
examine next is an exemplary illustration of these four defining
features of the social network perspective.

### Daly's School Leaders Dataset

Leadership network data were collected at two school districts over 3
consecutive years.

#### Research Questions:

1.  Is there a relationship between the frequency of collaboration
    between school leaders and how often they turn to each other to
    discuss issues of a confidential nature?
2.  Do school leaders prefer to collaborate with those with whom they
    have collaborated in the past, or is there some other reason?
3.  Does gender or some other individual attribute predicts confidential
    exchanges between school leaders, or does some previous relation
    have a stronger effect?
4.  Does collaboration between leaders explain one's level of trust in
    one's administrative colleagues?
5.  Can we distinguish among different groups of school leaders based on
    how frequently they collaborate, and if so, are these groupings
    related to the level at which they work (school versus district)?

#### Data Collection

For each consecutive year, school district leaders were invited to
complete a survey that collected individual:

-   **Demographic information** (e.g., gender, ethnicity, marital
    status, age, years of experiences),

-   **Network relationships types** (e.g., collaboration, confidential,
    energy, expertise, support you approach, support approach you,
    work-related issues, input, recognition, best practices, and
    innovation), efficacy, and trusting relationships.

-   **Frequency of interactions** they have with those nominated
    individuals on a four-point frequency scale ranging from 1 (the
    least frequent) to 4 (1--2 times a week).

-   **Leadership efficacy** items were designed based on the Principal
    Efficacy Scale used in Daly et al. (2011) and Tschannen-Moran and
    Gareis's (2004) studies. The efficacy scale includes 18 items rated
    on a 9-point Likert scale ranging from 1 (None at all) to 9 (A great
    deal).

-   **Trust** scale contains eight items rated on a 7-point Likert scale
    ranging from 1 (Strongly disagree) to 7 (Strongly agree) modified
    from Tschannen-Moran and Hoy (2003)

#### Analyses

Assumptions of independence Simulations...

-   **QAP/MR**: The quadratic assignment procedure (QAP) developed by
    Hubert (1987) and Krackhardt (1987b) tests the null hypothesis of no
    correlation between the two networks and adjusts for this dependence
    between networks by repeatedly permuting the order of rows and
    columns of one of the networks while keeping the other network
    intact. The QAP is based on regression models and permutation tests
    for valued (i.e., continuous) relational variables.

-   ERGM:

#### Key Findings

Findings here:

1.  **Is there a relationship between the frequency of collaboration
    between school leaders and how often they turn to each other to
    discuss issues of a confidential nature?** While collaboration in
    year 1 does not significantly predict collaboration in year 3,
    confidential exchanges in year 1 does, which suggests that
    collaboration among school leaders provides an important foundation
    for more sensitive, perhaps even deeper, relations (e.g.,
    confidential exchanges) at a later point in time
2.  

### **ðŸ‘‰ Your Turn** **â¤µ**

For our Unit 2 Walkthrough, we'll apply some of the same techniques used
by this study including some quick attempts at applying community
detection algorithms and automated text analysis using a few new
packages in R.

You are likely already familiar with some of the techniques and
approaches used in this study, however. For example, take a quick look
at the [*Explore the Networks*](https://www.hashtagcommoncore.com/#2-1)
section from Act 2: Central Actors and the Transmitters, Transceivers
and Transcenders identified by their analysis.

In the space below, type a brief response to the following questions:

1.  What network measures introduced in Unit 1 do think were used to
    identify these three types of "central actors"?

    -   YOUR RESPONSE HERE

Now check your response by looking at the [methods
page](https://www.hashtagcommoncore.com/project/methodology) of the
\#COMMONCORE Project.

1.  Were you correct?

    -   YOUR RESPONSE HERE

## 1b. Identify a Question(s)

Recall from above that our focus for this walkthrough is research
question 2:

> How are social media-enabled social networks changing the discourse in
> American politics that produces and sustains social policy?

For Unit 2, we are going to focus our questions on something a bit less
ambitious but inspired by this work:

1.  Who are the transmitters, transceivers, and transcenders in our
    Common Core Twitter network?
2.  What subgroups, or factions, exist in our network?
3.  Which actors in our network tend to be more opposed to the Common
    Core?

To address the latter question, we'll introduce a common text mining
technique in the Model section for gauging sentiment of social media
posts.

### **ðŸ‘‰ Your Turn** **â¤µ**

Based on what you know about networks and the context so far, what other
research questions might ask we ask in this context that a social
network perspective might be able to answer?

In the space below, type a brief response to the following questions:

-   YOUR RESPONSE HERE

## **1c. Set Up Project**

As highlighted inÂ [Chapter 6 of Data Science in Education Using
R](https://datascienceineducation.com/c06.html)Â (DSIEUR), one of the
first steps of every workflow should be to set up your "Project" within
RStudio. Recall that:

> A **Project** is the home for all of the files, images, reports, and
> code that are used in any given project

Since we are working in RStudio Cloud, a Project has already been set up
for you as indicated by the `unit-3.Rproj` file in your main directory
in the Files pane.

### Load Libraries

Recall that **packages**, or libraries, are shareable collections of R
code that can contain functions, data, and/or documentation and extend
the functionality of R. You can always check to see which packages have
already been installed and loaded into RStudio Cloud by looking at the
the Files, Plots, & Packages Pane in the lower right hand corner.

Let's go ahead and load the packages from Unit 1 since we'll be using
them again very soon:

```{r}
library(readxl)
library(tidyverse)
library(tidygraph)
library(ggraph)
```

#### statnet ðŸ“¦

![](img/statnetlogo.png){width="20%"}

Similar to the collection of packages contained in the {tidyverse}
package, the Statnet Team [@handcock:statnet] has developed a suite of R
packages for the management, exploration, statistical analysis,
simulation and vizualization of network data. The statistical modeling
framework used in {statnet} relies on Exponential-family Random Graph
Models (ERGMs).

Let's load the {statnet} package that we'll be using in just a bit to
accomplish all three of the goals listed above:

```{r}
library(statnet)
```

That's it! You're ready ready to start wrangling some tweets!!!

------------------------------------------------------------------------

# 2. WRANGLE

In general, data wrangling involves some combination of cleaning,
reshaping, transforming, and merging data [@wickham2016r]. The
importance of data wrangling is difficult to overstate, as it involves
the initial steps of going from the raw data to a dataset that can be
explored and modeled [@krumm2018].

For our data wrangling this week, we're keeping it simple since working
with network data is a bit of a departure from our working with
rectangular data frames. Our primary goals for Unit 1 are learning how
to:

a.  **Import Tweets**. In this section, we introduce
    theÂ `rtweet`Â package and some key functions to search for tweets or
    users of interest.

b.  **Create a Network Object**. Before performing network analyses,
    we'll need to convert our data frames into special data format for
    working with relational data.

c.  **Simplify Network**. Finally, we'll learn about a handy
    `simplify()` function in the {igraph} package for collapsing
    multiple ties between actors and removing "self-loops."

## 2a. Import Data

One of our primary goals is to replicate the ERGM analysis from Chapter
9: Network Data and Statistical Models [@carolan2014]. Specifically,
we'll aim to reproduce the results from **Table 9.4. Results of P1 and
P\*Analyses for the Dichotomized and Directed School Leaders
Confidential Exchanges Network Year 3.**

To do so, we'll need to import two Excel files from the Social Network
Analysis and Education companion site. The first file contains our edges
stored as a matrix (more on this later) and second file is a standard
rectangular data frame that contains attributes for each node, i.e.
school leader. These files are included in the data folder of your R
Studio project. A description of each file from the companion website
and a direct link to the file are included below:

1.  [**School Leaders Data Chapter
    9_d.**](https://studysites-sagepub-com.prox.lib.ncsu.edu/carolan/study/materials/datasets/99472_ds10.xls)Â This
    adjacency matrix reports on "confidential help" ties among 43 school
    leaders in year 3 of a three-year study. This is a directed valued
    (weighted) network measured on five-point scale ranging from 0 to 4,
    with higher values indicating more frequent collaborations (1--2
    times/week). These data are used throughout Chapter 9.

2.  [**School Leaders Data Chapter
    9_e.**](https://studysites-sagepub-com.prox.lib.ncsu.edu/carolan/study/materials/datasets/99472_ds11.xlsx)Â This
    rectangular matrix consists of four attribute vectors for 43 school
    leaders. Following the first ID column, the matrix includes an
    efficacy score, trust score, and indicators for whether one works at
    the district-level and is male (1 = yes, 0 = no). These attribute
    variables can be used as covariates in some of the statistical
    models covered in Chapter 9, including regression andÂ *p\**Â models.

Let's go ahead an import our matrix first as save as `adjacency_matrix`:

```{r}
adjacency_matrix <- read_excel("data/School Leaders Data Chapter 9_d.xlsx", col_names = FALSE) 
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Read in node attribute file and save as

```{r}
#YOUR CODE HERE
node_attributes <- read_excel("data/School Leaders Data Chapter 9_e.xlsx")
```

## 2c. Create Network Object

In Unit 1 we learned about the {igraph} package for preparing network
objects for analysis. In this section we introduce a new package that
builds upon the "the well-oiled machinery of igraph" but allows us to
use some of the familiar functions and syntax from other {tidyverse}
packages. In this walkthrough we won't recreate all the processes like
simplifying graphs and adding edge weights, but will demonstrate some
features similar to and in addition to those found in the {igraph}
package.

```{r}
class(adjacency_matrix)

matrix <- as.matrix(adjacency_matrix, header = T, row.names = 1)

class(matrix)
```

### Convert To Matrix

Before we can begin using many of the functions from the {tidygraph}
package for preparing and summarizing our Twitter network, we first need
to convert the data frames that we imported into a network object,
similar to what we did in Unit 1.

To do that, we will use the `tbl_graph()` function and include the
following arguments:

-   `edges =` expects a data frame, in our case `ties`, containing
    information about the edges in the graph. The nodes of each edge
    must either be in a `to` and `from` column, or in the two first
    columns like the data frame we provided.

-   `nodes =` expects a data frame, in our case `actors`, containing
    information about the nodes in the graph. If `to` and/or `from` are
    characters or names, like in our data frames, then they will be
    matched to the column named according to `node_key` in nodes, if it
    exists, or matched to the first column in the node list.

-   `directed =` specifies whether the constructed graph be directed and
    defaults to `TRUE` so we did not included it since our network is
    directed.

Let's go ahead and create our network graph, name it `network` and print
the output:

```{r}
network <- tbl_graph(matrix, nodes = node_attributes)

network
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a look at the output for our simple graph now and answer the
following questions:

1.  Are the numbers and names of nodes and actors consistent with our
    `actors` and `ties` data frames? What about the integers included in
    the `from` and `to` columns of the Edge Data?

    -   YOUR RESPONSE HERE

2.  What do you think "components" refers to? **Hint:** see Chapter 6 of
    [@carolan2014].

    -   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of data wrangling section and are ready
to start analysis! Before proceeding further, knit your document and
check to see if you encounter any errors.

------------------------------------------------------------------------

# 3. EXPLORE

As noted in the Getting Started Walkthrough and experienced in Unit 1,
exploratory data analysis involves the processes of describing your data
(such as by calculating the means and standard deviations of numeric
variables, or counting the frequency of categorical variables) and,
often, visualizing your data prior to modeling.

In Section 3, we use the {tidygraph} package for retrieving network
descriptives and introduce the {ggraph} package to create a network
visualization to help illustrate these metrics. Specifically, in this
section we'll learn to:

a.  **Examine Basic Descriptives**. We focus primarily on actors and
    edges in this walkthrough, including the edges wights we added in
    the previous section as well as node degree, and import and fairly
    intuitive measure of centrality.

b.  **Make a Sociogram**. Finally, we wrap up the explore phases by
    learning to plot a network and tweak key elements like the size,
    shape, and position of nodes and edges to better at communicating
    key findings.

## 3a. Examine Basic Descriptives

As we noted in Unit 1, many analyses of social networks are primarily
descriptive and aim to either represent the network's underlying social
structure through data-reduction techniques or to characterize network
properties through network measures.

### Centrality

#### Node Degree

Recall from Unit 1 that:

> **Degree** is the number of ties to and from an ego. In a directed
> network, in-degree is the number of ties received, whereas out-degree
> is the number of ties sent.

The {tidygraph} package has an unique function called `activate()` that
allows us to treat the nodes in our network object as if they were a
standard data frame that we can then apply standard tidyverse functions
to like `select()`, `filter()`, and `mutate()`.

The latter function, `mutate()`, we can use to create new variables for
nodes such as measures of degree, in-degree, and out-degree using the
`centrality_degree()` function in the {tidygraph} package.

Run the following code to add degree measures to each of our nodes and
print the output:

```{r}
network_1 <- network %>%
  activate(nodes) %>%
  mutate(degree = centrality_degree(mode = "all")) %>%
  mutate(in_degree = centrality_degree(mode = "in")) %>%
  mutate(out_degree = centrality_degree(mode = "out"))

network_1
```

We now see that these simple measures of centrality have been added to
the nodes in our network.

We can also use the `activate()` function combined with the
`data.frame()` function to extract our new measures to a separate data
frame so we inspect our nodes individually and create some summary
statistics using the handy `summary()` function.

```{r}
node_measures <- network_1 %>% 
  activate(nodes) %>%
  data.frame()

summary(node_measures)
```

Despite a dramatic size difference from our network in Unit 1, we see
that typical nodes in this network also have relatively few connections,
though there are a few exceptions.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that one of our questions guiding this
analysis was:

> Who are the transmitters, transceivers, and transcenders in our Common
> Core Twitter network?

Use the code chunk below to inspect our `node_measures` data frame and
answer the questions above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

#### Other Centrality Measures

In Chapter 7 of @carolan2014, noted that degree centrality does not take
into account indirect ties among all the alters in an ego's network.Â We
were also introduced to a few other measures of centrality commonly used
in network analysis and applied to educational contexts:

-   **Closeness** includes data about the relation between each pair of
    ego's named alters and is intuitively appealing in that being
    "close" to others and may indicate how quickly an actor can exchange
    something with others or be the first to receive information.

-   **Betweenness** captures how actors control or mediate the relations
    between pairs of actors that are not directly connected and is an
    important indicator of control over information exchange or resource
    flows within a network.

Carolyn also notes that in addition to these three common centrality
measures, many others have been developed and can be calculated in most
common social network-analysis software applications.Â In fact,
{tidygraph} includes many of these measures and includes various
`centrality_` functions for calculating node and edge centrality. Type
`?centrality` in your console below and hit enter to view them all.

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the code chunk below to add these closeness and betweenness measures
to our `network_1` data frame and save as `network_2`. I've included
some basic code to get your started.

```{r}
network_2 <- network_1
  #YOUR CODE HERE
  
```

## 3b. Make a Sociogram

If you recall from Unit 1, network visualization can be used for a
variety of purposes, ranging from highlighting key actors to even
serving as works of art. These visual representations of the actors and
their relations, i.e. the network, are called a **sociogram**. Actors
who are most central to the network, such as those with higher node
degrees, are usually placed in the center of the sociogram and their
ties are placed near them. In this section, we'll briefly introduce the
{ggraph} package for creating attractive network visualizations.

### ggraph ðŸ“¦

![](img/ggraph.png){width="20%"}

Created by the same developer as {tidygraph},
{[ggraph](https://ggraph.data-imaginist.com/index.html)} -- pronounced
gg-raph or g-giraffe hence the logo -- is an extension of
{[ggplot](https://ggplot2.tidyverse.org)} aimed at supporting relational
data structures such as networks, graphs, and trees. Both packages are
more modern and widely adopted approaches data visualization in R.

While ggraph builds upon the foundation of ggplot and its API, it comes
with its own self-contained set of geoms, facets, etc., as well as
adding the concept ofÂ *layouts*Â to the [grammar of
graphics](https://ggplot2-book.org/introduction.html?q=grammar#what-is-the-grammar-of-graphics),
i.e. the "gg" in ggplot and ggraph.

Let's go ahead and load the {ggraph} library:

```{r}
library(ggraph)
```

### A Simple Sociogram

Very similar to how functions in the tidyverse use the `%>%` operator to
"pipe" functions together and progressively wrangle data, ggraph and
ggrplot use the `+` operator to "layer" functions together to
progressively build graphs.

Let's start with the first and simplest function `ggraph` and supply our
network graph:

```{r}
ggraph(network_1)
```

As you can see, this didn't produce much. All this function does is take
care of setting up the network object to plot along with creating the
layout for the plot based on the graph and the specified layout passed
in.

#### Add Layout

Let's go ahead and include the layout argument, which in addition to
including its own unique layouts, can incorporate layouts form {igraph}
like `fr`.

```{r}
ggraph(network_1, layout = "fr")
```

#### Add Nodes

Still nothing but that is because we haven't added the nodes yet. Let's
do that:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point() 
```

Well, at least we have our nodes now!

The "geom" in the `geom_non_point()` functions stands for "Geometric
elements", or geoms for short, and represent what you actually see in
the plot.

These geoms can include aesthetics, or aes for short, such as `alpha`
for transparency, as well as `colour`, `shape` and `size`.

Let's now add some "aesthetics" to our points by including the `aes()`
function and arguments such as `size =` which we can set to our
`in_degree` measures:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point(aes(size = in_degree, 
                    alpha = out_degree, 
                    colour = degree))
```

And let's add some node text and labels while were at it since this is
not a very large network. Since node labels are a geometric element, we
can apply aesthetics to them as well. Let's also include the `repel =`
argument that when set toÂ `TRUE`Â will avoid overlapping text.

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = in_degree/2,
                     alpha = degree),
                 repel=TRUE)
```

Much better! Even without the edges, using size and opacity has helped
to illustrate some of the "transmitters," "trancievers" and
"transcenders."

#### Add Edges

Now, let's connect the dots and add some edges that include some arrows
1mm in length as well as an end cap to keep them from overlapping the
nodes:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3)
```

#### Add a Theme

Finally, let's add a **theme,** which controls the finer points of
display, like the font size and background color. The `theme_graph()`
function add a theme specially tuned for graph visualizations. This
function removes redundant elements in order to put focus on the data
and if you type `?theme_graph` in the console you will get a sense of
the level of fine tuning you can do if desired.

Let's add `theme_graph()` to our sociogram and call it good for now:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Try modifying the code below by tweaking the included function/arguments
or adding new ones for
[layouts](https://ggraph.data-imaginist.com/articles/Layouts.html),
[nodes](https://ggraph.data-imaginist.com/articles/Nodes.html), and
[edges](https://ggraph.data-imaginist.com/articles/Edges.html) to make
our plot either more "aesthetically pleasing" or more purposeful in what
it's trying to communicate.

There are no right or wrong answers, just have some fun trying out
different approaches!

```{r}
ggraph(network_1, layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of the Explore section and are ready to
learn a little about network modeling! Before proceeding further, knit
your document and check to see if you encounter any errors.

# 4. MODEL

As highlighted inÂ [Chapter 3 of Data Science in Education Using
R](https://datascienceineducation.com/c03.html), theÂ **Model**Â step of
the data science process entails "using statistical models, from simple
to complex, to understand trends and patterns in the data."

## 4a. Identify Groups

In Chapter 6: Groups and Positions in Complete Networks of SNA and
Education [@carolan2014] we were introduced to both "bottom up" and "top
down" approaches for identifying groups in a network, as well as why
researchers may be interested in exploring these groups. He also notes
that:

> Unlike most social science, the idea is to identify these groups
> through their relational data, not an exogenous attribute such as
> grade level, departmental affiliation, or years of experience.Â 

In this section, we'll briefly explore a "top down" approach to
identifying these groups through the use of community detection
algorithms.

### Community Detection

Similar to the range of functions included for calculating node and edge
centrality, the {tidygraph} package includes various clustering
functions provided by the {igraph} package introduced in Unit 1.

Also similar to calculating centrality measures, we need to `activate()`
our nodes first before applying these community detection algorithms to
assign our nodes to groups.

Run the following code and take a print our new network graph to the
console to take a quick look

```{r}
network_3 <- network_2 %>%
  activate(nodes) %>%
  mutate(group = group_infomap())
  
network_3
```

**Note:** Some of these algorithms are designed for directed graphs,
while others are for undirected graphs.

Now that we've assigned our nodes to a group, let's modify our sociogram
from above to color our nodes by group assignment and remove the
`alpha =` argument to make them a little easier to see.

```{r}
network_3 %>%
  ggraph(layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      colour = group)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the second question guiding this
analysis was:

> What subgroups, or factions, exist in our network?

Use the code chunk below to extract the group assignment data frame and
answer the question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

## 4b. Identify Sentiment

Sentiment analysis (also known asÂ opinion mining) is a text mining
technique used to "systematically identify, extract, quantify, and study
affective states and subjective information." In this section, we'll
introduce and apply the {vader} package to gain some insight into the
"lexical tendencies" of our tweets.

#### The vader Package ðŸ“¦

![](img/vader.jpeg){width="20%"}\

The {vader} package is for the Valence Aware Dictionary for sEntiment
Reasoning (VADER), a rule-based model for general sentiment analysis of
social media text and specifically attuned to measuring sentiment in
microblog-like contexts.

To learn more about the {vader} package and its development, take a look
at the article by Hutto and Gilbert (2014), [VADER: A Parsimonious
Rule-based Model forSentiment Analysis of Social Media
Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).

Let's go ahead and load the vader library:

```{r}
library(vader)
```

The {vader} package basically has just one function, `vader_df()` that
does one thing and expects just one column from a data frame. Let's give
VADER the our `ccss_tweets_6` data frame that we created earlier and
include the `$` operator to select for analysis our `text` column
containing our tweets.

```{r}
summary_vader <- vader_df(ties$text)

summary_vader
```

Hutto, C. & Gilbert, E. (2014) provide an excellent summary of the VADER
package on their [GitHub repository](scores) and I've copied and
explanation of the scores below:

-   TheÂ `compound`Â score is computed by summing the valence scores of
    each word in the lexicon, adjusted according to the rules, and then
    normalized to be between -1 (most extreme negative) and +1 (most
    extreme positive). This is the most useful metric if you want a
    single unidimensional measure of sentiment for a given sentence.
    Calling it a 'normalized, weighted composite score' is accurate.

**NOTE:**Â TheÂ `compound`Â score is the one most commonly used for
sentiment analysis by most researchers, including the authors.

Let's use the `inner_join()` function to add these sentiment scores
values back to our `ties` data frame and take a quick look:

```{r}
tweet_sentiment <-inner_join(summary_vader, 
                             ties,
                             by = "text")

tweet_sentiment
```

Now that we have these joined, we can take a quick look at the sentiment
.

```{r}
user_sentiment <- tweet_sentiment %>%
  group_by(sender) %>%
  summarise(sentiment = mean(compound))

user_sentiment
```

Note that we have effectively just created some new node and edge
"attributes" that could be incorporated into our network visualization
to potentially help understand why groups may have formed as they did.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the final question guiding this
analysis was:

> Which actors in our network tend to be more opposed to the Common
> Core?

Use the code chunk below to inspect our data frame and answer the
question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

------------------------------------------------------------------------

# 5. COMMUNICATE

For your Independent Analysis assignment for Unit 2 next week, you'll
create either a simple report or slide deck using an R Markdown document
just like this to share out some key findings from your analysis.
Regardless of whether you plan to talk us through your analysis and
findings with a presentation or walk us through with a brief written
report, your presentation or report should address the following
questions:

1.  **Purpose**. What question or questions are guiding your analysis?
    What did you hope to learn by answering these questions and why
    should your audience care about your findings?

2.  **Methods**. What data did you selected for analysis? What steps did
    you take took to prepare your data for analysis and what techniques
    you used to analyze your data? These should be fairly explicit with
    your embedded code.

3.  **Findings**. What did you ultimately find? How do your "data
    products" help to illustrate these findings? What conclusions can
    you draw from your analysis?

4.  **Discussion**. What were some of the strengths and weaknesses of
    your analysis? How might your audience use this information? How
    might you revisit or improve upon this analysis in the future?

### **ðŸ‘‰ Your Turn** **â¤µ**

Now that you've become more familiar with this dataset and the social
network perspective, what other aspects of this dataset, or a dataset
you are interested in exploring, could you investigate?

-   YOUR RESPONSE HERE

What specific research questions might you ask that would be helpful for
being understanding and improving learning, or the context in which the
data is collected? How could other approaches like sentiment analysis

-   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You've finished the Unit 2 Guided Walkthrough and are ready
for some independent analysis next week!

To complete this assignment, knit your document and send me an email at
sbkellog\@ncsu.edu letting me know you're all set.
