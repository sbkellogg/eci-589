---
title: 'Unit 2 Walkthrough: Hashtag Common Core'
subtitle: "ECI 589 Social Network Analysis and Education"
author: "Dr. Shaun Kellogg"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. PREPARE

Our Unit 2 Walkthrough: Hashtag Common Core is inspired by the work of
Jonathan Supovitz, Alan Daly, Miguel del Fresno and Christian Kolouch
who examined the intense debate surrounding the Common Core State
Standards education reform as it played out on Twitter. As noted on
their expansive and interactive website for the [\#COMMONCORE
Project](https://www.hashtagcommoncore.com), the Common Core was a major
education policy initiatives of the early 21st century that sought to
strengthen education systems across the United States through a set of
specific and challenging education standards. Although these standards
once enjoyed bipartisan support, we'll see later in this walkthrough how
these standards have become a political punching bag.

In addition to investigating tweets around the these controversial state
standards, our Unit 2 Walkthrough will cover the following topics
pertaining to each data-intensive workflow process:

1.  **Prepare**: Prior to analysis, we'll take a look at the context
    from which our data came, formulate some research questions, and get
    introduced the {rtweet} R package for using the Twitter API.

2.  **Wrangle**: Wrangling data entails the work of manipulating,
    cleaning, transforming, and merging data. In section 2 we will learn
    about the {tidygraph} package for creating network objects.

3.  **Explore**: In section 3, we use the {tidygraph} package and the
    companion {ggraph} package to calculate a range of centrality
    measures and learn how to illustrate some of these stats through
    network visualization.

4.  **Model**: We wrap up our analysis in Section 4 by introducing
    community detection and sentiment analysis algorithms for
    identifying groups and gauging sentiment about the common core.

5.  **Communicate**: We briefly reflect on our walkthrough in
    preparation for our independent analysis next week.

## 1a. Review the Research

Recall from [Social Network Analysis and Education: Theory, Methods &
Applications](https://methods.sagepub.com/book/social-network-analysis-and-education)
that Carolyn (2013) cited the following four features used by Freeman
(2004) to define the social network perspective:

1.  Social network analysis is **motivated by a relational intuition**
    based on ties connecting social actors.

2.  It is firmly **grounded in systematic empirical data**.

3.  It **makes** **use of graphic imagery** to represent actors and
    their relations with one another.

4.  It **relies** **on** **mathematical and/or computational models** to
    succinctly represent the complexity of social life.

The [\#COMMONCORE Project](https://www.hashtagcommoncore.com) that we'll
examine next is an exemplary illustration of these four defining
features of the social network perspective.

### The \#commoncore Project

![](img/commoncore.jpeg){width="50%"}

Supovitz, J., Daly, A.J., del Fresno, M., & Kolouch, C.
(2017).Â *\#commoncore Project.* Retrieved from
[http://www.hashtagcommoncore.com](http://www.hashtagcommoncore.com/).

#### Prologue

As noted by @supovitz2017commoncore, the Common Core State Standards
have been a "persistent flashpoint in the debate over the direction of
American education." The \#commoncore Project explores the Common Core
debate on Twitter using a combination of social network analyses and
psychological investigations which help to reveal both the underlying
social structure of the conversation and the motivations of the
participants.

The central question guiding this investigation was:

> How are social media-enabled social networks changing the discourse in
> American politics that produces and sustains social policy?

#### Data Sources & Analyses

The [methods
page](https://www.hashtagcommoncore.com/project/methodology) of the
\#COMMONCORE Project provides a detailed discussion of the data and
analyses used to arrive at the conclusions in *\#commoncore: How social
media is changing the politics of education*. Provided below is a
summary of how authors retrieved data from Twitter and the analyses
applied for each of the five acts in the website. I highly encourage you
to take a look at this section if you'd like to learn more about their
approach and in particular if you're unfamiliar with how users can
interact and communicate on Twitter.

#### Data Collection

To collect data on keywords related to the Common Core, the project used
a customized data collection tool developed by two of our co-authors,
Miguel del Fresno and Alan J. Daly, calledÂ *Social Runner
Lab^TM^*.Â Similar to an approach we'll use later in this walkthrough,
the authors downloaded data in real time directly from Twitter's
Application Programming Interface (API) based on tweets using specified
keywords, keyphrases, or hashtags and then restricted their analysis to
the following terms: *commoncore*, *ccss* and *stopcommoncore.* They
also captured Twitter profile names, or user names, as well as the
tweets, retweets, and mentions posted. Data included messages that are
public on twitter, but not private messages between individuals, nor
from accounts which users have made private or direct messages.

#### Analyses

In order to address their research question, the authors applied social
network analysis techniques in addition to qualitative and automated
text mining approaches. For social network analyses, each node is an
individual Twitter user (person, group, institution, etc.) and the
connection between each node is the tweet, retweet, or mention/reply.
After retrieving data from the Twitter API, the authors created a file
that could be analyzed in Gephi, an open-source software program which
depicts the relations as networks and provides metrics for describing
features of the network.

In addition to data visualization and network descriptives, the authors
examined group development and lexical tendencies among users. For the
former, they used a community detection algorithm to identify and
represent structural sub-communities, or factions (a "faction" in this
sense is a group with more ties within than across group even those
group boundaries are somewhat porous). For the latter, the authors used
the Linguistic Inquiry and Word Count (LIWC) lexicons to determine
psychological drive, monitor moods, diagnose their level of conviction,
and make inferences about thinking styles.

For a nice summary of the data used for the analysis, as well as the
samples of actors and tweets, the keywords, and the methods that were
utilized, see [Table 1. Data and Method for Each
Act](https://www.hashtagcommoncore.com/project/methodology) in the
Methods section of the \#commoncore website.

#### Key Findings

In the \#commoncore Project, analyses of almost 1 million tweets sent by
about 190,000 distinct actors over a period of 32 months revealed the
following:

-   In **Act 1**, **The Giant Network**, the authors identified five
    major sub-communities, or factions, in the Twitter debate
    surrounding the Common Core, including: (1) supporters of the Common
    Core, (2) opponents of the standards from inside education, and (3)
    opponents from outside of education.
-   InÂ **Act 2**, **Central Actors**, they noted that most of these
    participants were casual contributors -- almost 95% of them made
    fewer than 10 tweets in any given six-month period. They also
    distinguished between two types of influence on
    Twitter:Â *Transmitters*Â who tweeted a lot, regardless of the extent
    of their followership; andÂ *Transceivers*, those who gained their
    influence by being frequently retweeted and mentioned.
-   InÂ **Act 3**,Â **Key Events,**Â the authors identified issues driving
    the major spikes in the conversation, like when Secretary of
    Education Duncan spoke about white suburban moms' opposition to the
    Common Core, or the debate over the authorization of the Every
    Student Succeeds Act in November 2015. They also offended evidence
    of manufactured controversies spurred by sensationalizing minor
    issues and outright fake news stories.
-   InÂ **Act 4**, **Lexical Tendencies**, the authors examined the
    linguistic tendencies of the three major factions and found that
    Common Core supporters used the highest number of conviction words,
    tended to use more achievement-oriented language, and used more
    words associated with a formal and analytic thinking style. By
    contrast, opponents of the Common Core from within education tended
    to use more words associated with sadness, and used more narrative
    thinking style language. Opponents of the Common Core from outside
    of education made the highest use of words associated with peer
    affiliation, used the largest number of angry words, and exhibited
    the lowest level of conviction in their word choices.
-   In **Act 5**, **The Tweet Machine**, examined five frames that
    opponents of the Common Core used to appeal to values of particular
    subgroups including the government frame, business frame, war frame,
    experiment frame, and propaganda frame. By combining these
    constituencies, the authors illustrated how the Common Core
    developed a strong transpartisan coalition of opposition.

### **ðŸ‘‰ Your Turn** **â¤µ**

For our Unit 2 Walkthrough, we'll apply some of the same techniques used
by this study including some quick attempts at applying community
detection algorithms and automated text analysis using a few new
packages in R.

You are likely already familiar with some of the techniques and
approaches used in this study, however. For example, take a quick look
at the [*Explore the Networks*](https://www.hashtagcommoncore.com/#2-1)
section from Act 2: Central Actors and the Transmitters, Transceivers
and Transcenders identified by their analysis.

In the space below, type a brief response to the following questions:

1.  What network measures introduced in Unit 1 do think were used to
    identify these three types of "central actors"?

    -   YOUR RESPONSE HERE

Now check your response by looking at the [methods
page](https://www.hashtagcommoncore.com/project/methodology) of the
\#COMMONCORE Project.

1.  Were you correct?

    -   YOUR RESPONSE HERE

## 1b. Identify a Question(s)

Recall from above that the central question guiding the \#COMMONCORE
Project was:

> How are social media-enabled social networks changing the discourse in
> American politics that produces and sustains social policy?

For Unit 2, we are going to focus our questions on something a bit less
ambitious but inspired by this work:

1.  Who are the transmitters, transceivers, and transcenders in our
    Common Core Twitter network?
2.  What subgroups, or factions, exist in our network?
3.  Which actors in our network tend to be more opposed to the Common
    Core?

To address the latter question, we'll introduce a common text mining
technique in the Model section for gauging sentiment of social media
posts.

### **ðŸ‘‰ Your Turn** **â¤µ**

Based on what you know about networks and the context so far, what other
research questions might ask we ask in this context that a social
network perspective might be able to answer?

In the space below, type a brief response to the following questions:

-   YOUR RESPONSE HERE

## **1c. Set Up Project**

As highlighted inÂ [Chapter 6 of Data Science in Education Using
R](https://datascienceineducation.com/c06.html)Â (DSIEUR), one of the
first steps of every workflow should be to set up your "Project" within
RStudio. Recall that:

> A **Project** is the home for all of the files, images, reports, and
> code that are used in any given project

Since we are working in RStudio Cloud, a Project has already been set up
for you as indicated by the `eci-589.Rproj` file in your main directory
in the Files pane.

### Load Libraries

In Unit 1, we also learned about **packages**, or libraries, which are
shareable collections of R code that can contain functions, data, and/or
documentation and extend the functionality of R. You can always check to
see which packages have already been installed and loaded into RStudio
Cloud by looking at the the Files, Plots, & Packages Pane in the lower
right hand corner.

Let's go ahead and load the packages from Unit 1 since we'll be using
them again very soon:

```{r}
library(tidyverse)
library(igraph)
```

#### rtweet ðŸ“¦

![](img/rtweet.png){width="20%"}

Similar to approach used by @supovitz2017commoncore for downloading data
in directly from Twitter's Application Programming Interface (API), we
will be using {[rtweet](https://docs.ropensci.org/rtweet/)} package to
search for and download tweets based on specified keywords.

The `rtweet` package provides users a range of functions designed to
extract data from Twitter's REST and streaming APIs and has three main
goals:

1.  Formulate and send requests to Twitter's REST and stream APIs.

2.  Retrieve and iterate over returned data.

3.  Wrangling data into tidy structures.

Let's load the {rtweet} package that we'll be using in just a bit to
accomplish all three of the goals listed above:

```{r}
library(rtweet)
```

## 1c. AccessTwitter API

Before you can begin downloading data using the Twitter API, you'll
first need to create a Twitter App in your developer account. You are
not required to set up developer account for this course, but if you are
still interested in creating one, [these
instructions](https://moodle-courses2122.wolfware.ncsu.edu/mod/forum/view.php?id=119499)
outline the process and you can set one up in about 10 minutes.

If you are not interested in setting an account of your own, I have
provided you with temporary keys and tokens to download data from
Twitter and have also provided data for this walkthrough in the data
folder.

### **What is an API?**

For those new to the concept of an API, our very own Raleigh,
NC-basedÂ [Red
Hat](https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces)Â describes
an API as a means for one product or service communicate with other
products and services:

> APIs are sometimes thought of as contracts, with documentation that
> represents an agreement between parties: If party 1 sends a remote
> request structured a particular way, this is how party 2's software
> will respond.

As Duke University's Chris Bail notes, "APIs have become one of the most
important ways to access and transfer data online--- and increasingly
APIs can even analyze your data as well." Compared to screen-scraping
data from web pages, APIs are considerably easier to work with than the
HTML or XML data, ensure that data is collected legally, and with R
packages likeÂ `rtweet`, are logistically easier to work with, as we'll
soon see.

For a more in-depth explanation of APIs, how they work, and a preview of
some of theÂ `rtweet`Â functions we'll be using this summer, I highly
recommend reading through Chris Bail'sÂ [Application Programming
Interfaces in
R](https://sicss.io/2020/materials/day2-digital-trace-data/apis/rmarkdown/Application_Programming_interfaces.html)Â tutorial.

### Authorize RStudio

In order to authorize R to use your Twitter App to retrieve data, you'll
need to create a personal Twitter token by completing the following
steps:

1.  Navigate to
    [developer.twitter.com/en/apps](https://developer.twitter.com/en/apps)
    and select your Twitter app
2.  Click the tab labeled `Keys and tokens` to retrieve your keys.
3.  Locate the `Consumer API keys` (aka "API Secret").

![](img/create-app-6.png){width="80%"}

3.  Scroll down to `Access token & access token secret` and click
    `Create`

![](img/create-app-7.png){width="80%"}

4.  Locate the following code into the R Script file named
    `twitter-auth.R` located in files pane, replace the four fake keys
    with your own, and pass them along to `create_token()` function.

```{r api-keys, eval=FALSE}
## store api keys (these are fake example values; replace with your own keys)
app_name <- "Text Mining in Education"
api_key <- "afYS4vbIlPAj096E60c4W1fiK"
api_secret_key <- "bI91kqnqFoNCrZFbsjAWHD4gJ91LQAhdCJXCj3yscfuULtNkuu"
access_token <- "9551451262-wK2EmA942kxZYIwa5LMKZoQA4Xc2uyIiEwu2YXL"
access_token_secret <- "9vpiSGKg1fIPQtxc5d5ESiFlZQpfbknEN1f1m2xe5byw7"

## authenticate via web browser
token <- create_token(
  app = app_name,
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

**Note**: these keys are named secret for a reason. We store these up in
a separate R script file rather than in a R Markdown file that you will
eventually share. If you do not have your own keys, temporary keys have
been provided for you in the `twitter-auth.r` file in your Files pane.

#### Check Authorization

The `create_token()` function should automatically save your token as an
environment variable. So next time you start an R session -- on the same
machine -- rtweet should ðŸ¤ž automatically find your token.

5.  To make sure it works, you can run the following code and check to
    make sure the app name and `api_key` match those on your dev
    account.

```{r get-token}
## check to see if the token is loaded
get_token()
```

That's it! You're ready ready to start wrangling some tweets!!!

------------------------------------------------------------------------

# 2. WRANGLE

In general, data wrangling involves some combination of cleaning,
reshaping, transforming, and merging data [@wickham2016r]. The
importance of data wrangling is difficult to overstate, as it involves
the initial steps of going from the raw data to a dataset that can be
explored and modeled [@krumm2018].

For our data wrangling this week, we're keeping it simple since working
with network data is a bit of a departure from our working with
rectangular data frames. Our primary goals for Unit 1 are learning how
to:

a.  **Import Tweets**. In this section, we introduce
    theÂ `rtweet`Â package and some key functions to search for tweets or
    users of interest.

b.  **Create a Network Object**. Before performing network analyses,
    we'll need to convert our data frames into special data format for
    working with relational data.

c.  **Simplify Network**. Finally, we'll learn about a handy
    `simplify()` function in the {igraph} package for collapsing
    multiple ties between actors and removing "self-loops."

## 2a. Import Tweets

The Import Tweets section introduces the following functions from the
`rtweet` package for reading Twitter data into R:

-   [`search_tweets()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets)
    Pulls up to 18,000 tweets from the last 6-9 days matching provided
    search terms.
-   [`search_tweets2()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets)
    Returns data from **multiple** search queries.
-   [`get_timelines()`](https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets)
    Returns up to 3,200 tweets of one or more specified Twitter users.

### Search Tweets

Since one of our goals for this walkthrough is a very crude replication
of the study by @supovitz2017commoncore, let's begin by introducing the
`search_tweets()` function to try reading into R 5,000 tweets containing
the commoncore hashtag and store as a new data frame `ccss_tweets`.

Type or copy the following code into your R script or console and run:

```{r}
ccss_tweets <- search_tweets(q = "#commoncore", n=5000)
```

You likely saw in the output or console a download message like the one
below that stopped well short of 100%. That is pretty typical and just
indicates the API returned fewer tweets than requested.

Note also that the first argument `q =` that the `search_tweets()`
function expects is the search term included in quotation marks and that
`n =` specifies the maximum number of tweets

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the code chunk below to view your new `ccss_tweets` data frame using
one of the [various view
methods](https://cmdlinetips.com/2020/06/get-a-peek-at-dataframe-in-r/)
for looking at your data in R:

```{r}
#YOUR CODE HERE
```

1.  How many tweets did our query using the Twitter API actually return?
    How many variables?

    -   YOUR RESPONSE HERE

2.  Why do you think our query pulled in far less than 5,000 tweets
    requested? **Hint:** Type `?search_tweets` in your console to see
    the support documentation for this function.

    -   YOUR RESPONSE HERE

3.  Does our query also include retweets? How do you know?

    -   YOUR RESPONSE HERE

### Remove Retweets

While the authors explicitly mentioned including retweets when
constructing their network, `search_tweets()` function has an argument
that includes retweets by default, but can be changed to exclude
retweets if desired.

Even though we will want to include retweets to construct our final
network, let's test out the `include_rts =` argument and set it to
`FALSE` to remove any retweets:

```{r}
ccss_tweets_1 <- search_tweets("#commoncore", 
                                   n=500, 
                                   include_rts = FALSE)
```

Take a look at your environment pane and see how many tweets were
excluded by adding this argument.

### Using the OR Operator

If you recall from the Prepare section, the authors accessed tweets and
user information from using multiple search terms, including:
*commoncore*, *ccss* and *stopcommoncore.*

Let's modify our query using the `OR` operator to also include "ccss"
and "stopcommoncore" so it will return tweets containing either any of
these three terms. And since we've included additional search terms
let's also increase the number of requested tweets to 5,000.

```{r}
ccss_tweets_2 <- search_tweets(q = "#commoncore OR #stopcommoncore OR #ccss",
                               n=5000)
```

Wow, those additional search terms dramatically increased the number of
tweets downloaded!

### **ðŸ‘‰ Your Turn** **â¤µ**

Try including both search terms but excluding the `OR` operator to
answer the following question:

1.  Does excluding the `OR` operator return more tweets, the same number
    of tweets, or fewer tweets? Why?

    -   YOUR RESPONSE HERE

2.  What other useful arguments does the `search_tweet()` function
    contain? Try adding one and see what happens.

    -   YOUR RESPONSE HERE

**Hint:** Use the `?search_tweets` help function to learn more about the
`q` argument and other arguments for composing search queries.

### Filter for English Tweets

If you haven't already done so, take a look at the tweets in our
`ccss_tweets_2` data frame. You probably noticed that many of these
tweets were in Spanish, as indicated by the "es" in the `lang` column of
our data frame.

Let's take a quick count of the number of tweets in each language of our
data frame by sending our `ccss_tweets_2` data frame to the `count()`
function using the `%>%` operator, specifying `lang` as the column whose
values we want to count and sorting from high to low:

```{r}
ccss_tweets_2 %>%
  count(lang, sort = TRUE)
```

Surprisingly, the majority of these tweets are in Spanish. For the
purpose of both limiting the size of our network for this analysis and
also because we are interested in doing some quick sentiment analysis
towards the end of our walkthrough, let's limit our tweets to just those
in the English language.

To download only English tweets, we will need include the `lang =`
argument and set it to "en":

```{r}
ccss_tweets_3 <- search_tweets(q = "#commoncore OR #stopcommoncore OR #ccss",
                               n=5000,
                               lang = "en")
```

#### Save Tweets to File

Finally, let's save our tweet files to use in later exercises since
tweets have a tendency to change every minute.

```{r, eval=FALSE}
write_csv(ccss_tweets_3, "data/ccss-tweets.csv")
```

Uh oh! We received an error because data stored in the `hashtags` column
is not stored as typical character, number, logical, or date data type,
but rather it is stored as a "list."

Use the `glimpse()` function to see which other columns store data as
lists:

```{r}
glimpse(ccss_tweets_3)
```

Since columns containing lists can not be stored in flat files like
.csv, we'll need to "flatten" our data in these columns using the aptly
named `flatten()` function from the {rtweets} package:

```{r}
ccss_tweets_4 <- rtweet::flatten(ccss_tweets_3)
```

Note that we included the package name followed by a double colon
`rtweet::` when calling our function to avoid confusion with other
functions using the same name.

Now we can use the `read_csv()` function to save our data frame to our
folder.

```{r}
write_csv(ccss_tweets_4, "data/ccss-tweets-5.csv")
```

Check your data folder to make sure this file has been save. You will
also see a `ccss-tweets-5.csv` file which I have save earlier and which
we will use in the next section to ensure we all have the same results.

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the `search_tweets` function to create your own custom query for a
twitter hashtag or topic(s) of interest and save as a .csv file in the
data folder.

```{r}
#YOUR CODE HERE
```

**Note:** You may want to use this data for your independent analysis
next week.

#### Other Useful Queries

For your independent analysis, you may be interest in exploring posts by
specific users rather than topics, key words, or hashtags. Yes, there is
a function for that too!

For example, let's create another list containing the usernames of me
and some of my colleagues at the Friday Institute using the `c()`
function again and use the `get_timelines()` function to get the most
recent tweets from each of those users:

```{r}
fi <- c("sbkellogg", "toosweetgeek", "haspires", "tarheel93", "drcallie_tweets", "AlexDreier")

fi_tweets <- fi %>%
  get_timelines(include_rts=FALSE)
```

And let's use the `sample_n()` function from the `dplyr` package to pick
10 random tweets and use `select()` to select and view just the
`screenname` and `text` columns that contains the user and the content
of their post:

```{r}
sample_n(fi_tweets, 10) %>%
  select(screen_name, text)
```

We've only scratched the surface of the number of functions available in
the `rtweets` package for searching Twitter. Use the following function
to take a look at other ways you can query data using the Twitter API:

```{r, eval=F}
vignette("intro", package="rtweet")
```

### **ðŸ‘‰ Your Turn** **â¤µ**

To conclude Section 2a, try one of the following search functions from
the `rtweet` vignette:

1.  `get_timelines()` Get the most recent 3,200 tweets from users.
2.  `stream_tweets()` Randomly sample (approximately 1%) from the live
    stream of all tweets.
3.  `get_friends()` Retrieve a list of all the accounts a user follows.
4.  `get_followers()` Retrieve a list of the accounts following a user.
5.  `get_favorites()` Get the most recently favorited statuses by a
    user.
6.  `get_trends()` Discover what's currently trending in a city.
7.  `search_users()` Search for 1,000 users with the specific hashtag in
    their profile bios.

```{r}
#YOUR CODE HERE
```

## 2b. Create Edgelist from Tweets

Recall from Unit 1 that an edge-list that contains information about
each tie, or relation between two actors in a network. In our previous
walkthrough, a "tie" is a reply by one participant in the discussion
forum to the post of another participant -- or in some cases to their
own post. For our analysis of tweets, we'll use the same approach used
by @supovitz2017commoncore. Specifically, each node is an individual
Twitter user (person, group, institution, etc.) and the connection
between each node is the tweet, retweet, or mention/reply.

### Import Saved Tweets

Before we proceed, let's first use the `read_csv()` function from the
{readr} package introduced in the Getting Started walkthrough to read
the `ccss-tweets-5.csv` file saved in or data folder so we are all
working with the same data:

```{r}
ccss_tweets_5 <- read_csv("data/ccss-tweets-5.csv")
```

### **ðŸ‘‰ Your Turn** **â¤µ**

As highlighted in @estrellado2020e, cleaning and tidying network data
can be even more challenging than for other data sources, network data
often includes variables about both individuals and their relationships.
This is especially true of our data downloaded using the Twitter API.

Take a look at the data we just imported using one of your favorite
methods for inspecting data and in the space below, identify the columns
you think could be used to construct an edge list.

-   YOUR RESPONSE HERE

### Extract Edges and Nodes

If one of the columns you indicated in your response above included
`screen_name` nice work! You may also have noticed that the
`mentions_screen_name` column also includes the names of those in the
reply column.

#### Create Edgelist

Recall from Unit 1 that the first two columns in an edgelist should
include the nodes that make up a tie or dyad. Since the only two columns
we need to construct our edgelist is the `screen_name` of the tweet
author and the screen names of those included in the mentions, let's
`relocate()` and rename those columns and then `select()` those columns
along with any attributes that we think might be useful for analysis
later on, like the timestamp or content of the tweet, i.e. `text`.

```{r}
ties_1 <-  ccss_tweets_5 %>%
  relocate(sender = screen_name, # rename scree_name to sender
           target = mentions_screen_name) %>% # rename to receiver
  select(sender,
         target,
         created_at,
         text)
```

Our edgelist with attributes is almost ready, but we have a couple
issues we still need to deal with.

As you may have noticed, our receiver column contains the names of
multiple Twitter users, but a dyad or tie can only be between two actors
or nodes.

#### "Unnest" User Names

In order to place each target user in a separate row that corresponds
with each sender of the tweet, we will need to "unnest" these names
using a package that those who took the ECI 588 Text Mining course will
be very familiar with.

#### tidytext ðŸ“¦

![](img/tidytext.png){width="20%"}

The {tidytext} package helps to convert text into data frames of
individual words, making it easy to to manipulate, summarize, and
visualize text using using familiar functions form the {tidyverse}
collection of packages.

Let's go ahead and load the {tidytext} package:

```{r}
library(tidytext)
```

For a more comprehensive introduction to the `tidytext` package, we
cannot recommend enough the free online book, *Text Mining with R: A
Tidy Approach* [@silge2017text]. If you're interested in pursuing text
analysis beyond this walkthrough, this will be a go to reference.

Let's go ahead and apply the \`un

```{r}
ties_2 <- ties_1 %>%
  unnest_tokens(input = target,
                output = receiver,
                to_lower = FALSE) %>%
  relocate(sender, receiver)
```

There is A LOT to unpack with this function. First notice
thatÂ `unnest_tokens`Â expects a data frame as the first argument, which
we supplied using teh %\>% operator, followed by two column names. The
first is an input column that the text comes from, `target` in this
case, which we want to "unnest" into separate rows. Also notice:

-   Other columns, such asÂ `sender`Â andÂ `text`, are retained.

-   By default, words are changed to lowercase, which makes them easier
    to compare or combine with other datasets. However, we can set
    theÂ `to_lower = FALSE`Â argument to turn off this behavior since
    capitalization is important for our user names.

Finally, we'll introduce the `drop_na()` function to remove the rows
with missing values from our `receiver` column since they are incomplete
dyads. We'll include these actors in our network later as part of our
nodelist in just a bit.

Let's drop our missing values and save as `ties` for our final data
frame

```{r}
ties <- ties_2 %>%
  drop_na(receiver)
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a quick look at our final edgelist and answer the following
question:

How many edges our in our CCSS network?

-   YOUR RESPONSE HERE

Using the tweets you downloaded with your own custom query from above,
create an edgelist with any desired edge attributes.

```{r}
# YOUR CODE HERE
```

### Node Attributes

The second file we need to create is a data frame that contains all the
nodes or actors in our network.

Regardless, let's read in our node attribute file and take a look at the
`actors` and their attributes included in our dataset:

```{r}
actors_1 <- ties_2 %>%
  pivot_longer(cols = sender:receiver, 
               names_to = "nodes",
               values_to = "screen_name")
```

Since

```{r}
actors <- actors_1 %>%
  select(screen_name) %>%
  distinct() %>%
  drop_na()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a quick look at our final `actors` data frame and answer the
following question:

How many nodes our in our CCSS network?

-   YOUR RESPONSE HERE

Using the tweets you downloaded with your own custom query from above,
create an nodelist containing all the unique actors in your network.

```{r}
# YOUR CODE HERE
```

## 2c. Create Network Object

In Unit 1 we learned about the {igraph} package for preparing network
objects for analysis. In this section we introduce a new package that
builds upon the "the well-oiled machinery of igraph" but allows us to
use some of the familiar functions and syntax from other {tidyverse}
packages. In this walkthrough we won't recreate all the processes like
simplifying graphs and adding edge weights, but will demonstrate some
features similar to and in addition to those found in the {igraph}
package.

### tidygraph ðŸ“¦

![](img/tidygraph.png){width="20%"}

The {[tidygraph](https://tidygraph.data-imaginist.com)} package is a
huge package that exports 280 different functions and methods. It more
or less wraps the full functionality ofÂ `igraph`Â in a tidy API giving
you access to almost all of theÂ `dplyr`Â verbs plus a few more, developed
for use with relational data. While network data itself is not tidy, it
can be envisioned as two tidy tables, one for node data and one for edge
data. The {tidygraph} package provides a way to switch between the two
tables andÂ `dplyr`Â verbs to manipulate them. Furthermore it provides
access to a lot of graph algorithms with return values that facilitate
their use in a tidy workflow.

Let's go ahead and load the {tidygraph} library:

```{r}
library(tidygraph)
```

### Combine Edges & Nodes

Before we can begin using many of the functions from the {tidygraph}
package for preparing and summarizing our Twitter network, we first need
to convert the data frames that we imported into a network object,
similar to what we did in Unit 1.

To do that, we will use the `tbl_graph()` function and include the
following arguments:

-   `edges =` expects a data frame, in our case `ties`, containing
    information about the edges in the graph. The nodes of each edge
    must either be in a `to` and `from` column, or in the two first
    columns like the data frame we provided.

-   `nodes =` expects a data frame, in our case `actors`, containing
    information about the nodes in the graph. If `to` and/or `from` are
    characters or names, like in our data frames, then they will be
    matched to the column named according to `node_key` in nodes, if it
    exists, or matched to the first column in the node list.

-   `directed =` specifies whether the constructed graph be directed and
    defaults to `TRUE` so we did not included it since our network is
    directed.

Let's go ahead and create our network graph, name it `network` and print
the output:

```{r}
network <- tbl_graph(edges = ties, 
                     nodes = actors)

network
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Take a look at the output for our simple graph now and answer the
following questions:

1.  Are the numbers and names of nodes and actors consistent with our
    `actors` and `ties` data frames? What about the integers included in
    the `from` and `to` columns of the Edge Data?

    -   YOUR RESPONSE HERE

2.  What do you think "components" refers to? **Hint:** see Chapter 6 of
    [@carolan2014].

    -   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of data wrangling section and are ready
to start analysis! Before proceeding further, knit your document and
check to see if you encounter any errors.

------------------------------------------------------------------------

# 3. EXPLORE

As noted in the Getting Started Walkthrough and experienced in Unit 1,
exploratory data analysis involves the processes of describing your data
(such as by calculating the means and standard deviations of numeric
variables, or counting the frequency of categorical variables) and,
often, visualizing your data prior to modeling.

In Section 3, we use the {tidygraph} package for retrieving network
descriptives and introduce the {ggraph} package to create a network
visualization to help illustrate these metrics. Specifically, in this
section we'll learn to:

a.  **Examine Basic Descriptives**. We focus primarily on actors and
    edges in this walkthrough, including the edges wights we added in
    the previous section as well as node degree, and import and fairly
    intuitive measure of centrality.

b.  **Make a Sociogram**. Finally, we wrap up the explore phases by
    learning to plot a network and tweak key elements like the size,
    shape, and position of nodes and edges to better at communicating
    key findings.

## 3a. Examine Basic Descriptives

As we noted in Unit 1, many analyses of social networks are primarily
descriptive and aim to either represent the network's underlying social
structure through data-reduction techniques or to characterize network
properties through network measures.

### Centrality

#### Node Degree

Recall from Unit 1 that:

> **Degree** is the number of ties to and from an ego. In a directed
> network, in-degree is the number of ties received, whereas out-degree
> is the number of ties sent.

The {tidygraph} package has an unique function called `activate()` that
allows us to treat the nodes in our network object as if they were a
standard data frame that we can then apply standard tidyverse functions
to like `select()`, `filter()`, and `mutate()`.

The latter function, `mutate()`, we can use to create new variables for
nodes such as measures of degree, in-degree, and out-degree using the
`centrality_degree()` function in the {tidygraph} package.

Run the following code to add degree measures to each of our nodes and
print the output:

```{r}
network_1 <- network %>%
  activate(nodes) %>%
  mutate(degree = centrality_degree(mode = "all")) %>%
  mutate(in_degree = centrality_degree(mode = "in")) %>%
  mutate(out_degree = centrality_degree(mode = "out"))

network_1
```

We now see that these simple measures of centrality have been added to
the nodes in our network.

We can also use the `activate()` function combined with the
`data.frame()` function to extract our new measures to a separate data
frame so we inspect our nodes individually and create some summary
statistics using the handy `summary()` function.

```{r}
node_measures <- network_1 %>% 
  activate(nodes) %>%
  data.frame()

summary(node_measures)
```

Despite a dramatic size difference from our network in Unit 1, we see
that typical nodes in this network also have relatively few connections,
though there are a few exceptions.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that one of our questions guiding this
analysis was:

> Who are the transmitters, transceivers, and transcenders in our Common
> Core Twitter network?

Use the code chunk below to inspect our `node_measures` data frame and
answer the questions above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

#### Other Centrality Measures

In Chapter 7 of @carolan2014, noted that degree centrality does not take
into account indirect ties among all the alters in an ego's network.Â We
were also introduced to a few other measures of centrality commonly used
in network analysis and applied to educational contexts:

-   **Closeness** includes data about the relation between each pair of
    ego's named alters and is intuitively appealing in that being
    "close" to others and may indicate how quickly an actor can exchange
    something with others or be the first to receive information.

-   **Betweenness** captures how actors control or mediate the relations
    between pairs of actors that are not directly connected and is an
    important indicator of control over information exchange or resource
    flows within a network.

Carolyn also notes that in addition to these three common centrality
measures, many others have been developed and can be calculated in most
common social network-analysis software applications.Â In fact,
{tidygraph} includes many of these measures and includes various
`centrality_` functions for calculating node and edge centrality. Type
`?centrality` in your console below and hit enter to view them all.

### **ðŸ‘‰ Your Turn** **â¤µ**

Use the code chunk below to add these closeness and betweenness measures
to our `network_1` data frame and save as `network_2`. I've included
some basic code to get your started.

```{r}
network_2 <- network_1
  #YOUR CODE HERE
  
```

## 3b. Make a Sociogram

If you recall from Unit 1, network visualization can be used for a
variety of purposes, ranging from highlighting key actors to even
serving as works of art. These visual representations of the actors and
their relations, i.e. the network, are called a **sociogram**. Actors
who are most central to the network, such as those with higher node
degrees, are usually placed in the center of the sociogram and their
ties are placed near them. In this section, we'll briefly introduce the
{ggraph} package for creating attractive network visualizations.

### ggraph ðŸ“¦

![](img/ggraph.png){width="20%"}

Created by the same developer as {tidygraph},
{[ggraph](https://ggraph.data-imaginist.com/index.html)} -- pronounced
gg-raph or g-giraffe hence the logo -- is an extension of
{[ggplot](https://ggplot2.tidyverse.org)} aimed at supporting relational
data structures such as networks, graphs, and trees. Both packages are
more modern and widely adopted approaches data visualization in R.

While ggraph builds upon the foundation of ggplot and its API, it comes
with its own self-contained set of geoms, facets, etc., as well as
adding the concept ofÂ *layouts*Â to the [grammar of
graphics](https://ggplot2-book.org/introduction.html?q=grammar#what-is-the-grammar-of-graphics),
i.e. the "gg" in ggplot and ggraph.

Let's go ahead and load the {ggraph} library:

```{r}
library(ggraph)
```

### A Simple Sociogram

Very similar to how functions in the tidyverse use the `%>%` operator to
"pipe" functions together and progressively wrangle data, ggraph and
ggrplot use the `+` operator to "layer" functions together to
progressively build graphs.

Let's start with the first and simplest function `ggraph` and supply our
network graph:

```{r}
ggraph(network_1)
```

As you can see, this didn't produce much. All this function does is take
care of setting up the network object to plot along with creating the
layout for the plot based on the graph and the specified layout passed
in.

#### Add Layout

Let's go ahead and include the layout argument, which in addition to
including its own unique layouts, can incorporate layouts form {igraph}
like `fr`.

```{r}
ggraph(network_1, layout = "fr")
```

#### Add Nodes

Still nothing but that is because we haven't added the nodes yet. Let's
do that:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point() 
```

Well, at least we have our nodes now!

The "geom" in the `geom_non_point()` functions stands for "Geometric
elements", or geoms for short, and represent what you actually see in
the plot.

These geoms can include aesthetics, or aes for short, such as `alpha`
for transparency, as well as `colour`, `shape` and `size`.

Let's now add some "aesthetics" to our points by including the `aes()`
function and arguments such as `size =` which we can set to our
`in_degree` measures:

```{r}
ggraph(network_1, layout = "fr") + 
geom_node_point(aes(size = in_degree, 
                    alpha = out_degree, 
                    colour = degree))
```

And let's add some node text and labels while were at it since this is
not a very large network. Since node labels are a geometric element, we
can apply aesthetics to them as well. Let's also include the `repel =`
argument that when set toÂ `TRUE`Â will avoid overlapping text.

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = in_degree/2,
                     alpha = degree),
                 repel=TRUE)
```

Much better! Even without the edges, using size and opacity has helped
to illustrate some of the "transmitters," "trancievers" and
"transcenders."

#### Add Edges

Now, let's connect the dots and add some edges that include some arrows
1mm in length as well as an end cap to keep them from overlapping the
nodes:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3)
```

#### Add a Theme

Finally, let's add a **theme,** which controls the finer points of
display, like the font size and background color. The `theme_graph()`
function add a theme specially tuned for graph visualizations. This
function removes redundant elements in order to put focus on the data
and if you type `?theme_graph` in the console you will get a sense of
the level of fine tuning you can do if desired.

Let's add `theme_graph()` to our sociogram and call it good for now:

```{r}
ggraph(network_1, layout = "fr") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Try modifying the code below by tweaking the included function/arguments
or adding new ones for
[layouts](https://ggraph.data-imaginist.com/articles/Layouts.html),
[nodes](https://ggraph.data-imaginist.com/articles/Nodes.html), and
[edges](https://ggraph.data-imaginist.com/articles/Edges.html) to make
our plot either more "aesthetically pleasing" or more purposeful in what
it's trying to communicate.

There are no right or wrong answers, just have some fun trying out
different approaches!

```{r}
ggraph(network_1, layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      alpha = out_degree, 
                      colour = degree)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### ðŸ§¶ Knit & Check âœ…

Congrats! You made it to the end of the Explore section and are ready to
learn a little about network modeling! Before proceeding further, knit
your document and check to see if you encounter any errors.

# 4. MODEL

As highlighted inÂ [Chapter 3 of Data Science in Education Using
R](https://datascienceineducation.com/c03.html), theÂ **Model**Â step of
the data science process entails "using statistical models, from simple
to complex, to understand trends and patterns in the data."

## 4a. Identify Groups

In Chapter 6: Groups and Positions in Complete Networks of SNA and
Education [@carolan2014] we were introduced to both "bottom up" and "top
down" approaches for identifying groups in a network, as well as why
researchers may be interested in exploring these groups. He also notes
that:

> Unlike most social science, the idea is to identify these groups
> through their relational data, not an exogenous attribute such as
> grade level, departmental affiliation, or years of experience.Â 

In this section, we'll briefly explore a "top down" approach to
identifying these groups through the use of community detection
algorithms.

### Community Detection

Similar to the range of functions included for calculating node and edge
centrality, the {tidygraph} package includes various clustering
functions provided by the {igraph} package introduced in Unit 1.

Also similar to calculating centrality measures, we need to `activate()`
our nodes first before applying these community detection algorithms to
assign our nodes to groups.

Run the following code and take a print our new network graph to the
console to take a quick look

```{r}
network_3 <- network_2 %>%
  activate(nodes) %>%
  mutate(group = group_infomap())
  
network_3
```

**Note:** Some of these algorithms are designed for directed graphs,
while others are for undirected graphs.

Now that we've assigned our nodes to a group, let's modify our sociogram
from above to color our nodes by group assignment and remove the
`alpha =` argument to make them a little easier to see.

```{r}
network_3 %>%
  ggraph(layout = "kk") + 
  geom_node_point(aes(size = in_degree, 
                      colour = group)) +
  geom_node_text(aes(label = screen_name, 
                     size = degree/2,
                     alpha = degree), 
                     repel=TRUE) +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 end_cap = circle(3, 'mm'),
                 alpha = .3) + 
  theme_graph()
```

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the second question guiding this
analysis was:

> What subgroups, or factions, exist in our network?

Use the code chunk below to extract the group assignment data frame and
answer the question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

## 4b. Identify Sentiment

Sentiment analysis (also known asÂ opinion mining) is a text mining
technique used to "systematically identify, extract, quantify, and study
affective states and subjective information." In this section, we'll
introduce and apply the {vader} package to gain some insight into the
"lexical tendencies" of our tweets.

#### The vader Package ðŸ“¦

![](img/vader.jpeg){width="20%"}\

The {vader} package is for the Valence Aware Dictionary for sEntiment
Reasoning (VADER), a rule-based model for general sentiment analysis of
social media text and specifically attuned to measuring sentiment in
microblog-like contexts.

To learn more about the {vader} package and its development, take a look
at the article by Hutto and Gilbert (2014), [VADER: A Parsimonious
Rule-based Model forSentiment Analysis of Social Media
Text](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).

Let's go ahead and load the vader library:

```{r}
library(vader)
```

The {vader} package basically has just one function, `vader_df()` that
does one thing and expects just one column from a data frame. Let's give
VADER the our `ccss_tweets_6` data frame that we created earlier and
include the `$` operator to select for analysis our `text` column
containing our tweets.

```{r}
summary_vader <- vader_df(ties$text)

summary_vader
```

Hutto, C. & Gilbert, E. (2014) provide an excellent summary of the VADER
package on their [GitHub repository](scores) and I've copied and
explanation of the scores below:

-   TheÂ `compound`Â score is computed by summing the valence scores of
    each word in the lexicon, adjusted according to the rules, and then
    normalized to be between -1 (most extreme negative) and +1 (most
    extreme positive). This is the most useful metric if you want a
    single unidimensional measure of sentiment for a given sentence.
    Calling it a 'normalized, weighted composite score' is accurate.

**NOTE:**Â TheÂ `compound`Â score is the one most commonly used for
sentiment analysis by most researchers, including the authors.

Let's use the `inner_join()` function to add these sentiment scores
values back to our `ties` data frame and take a quick look:

```{r}
tweet_sentiment <-inner_join(summary_vader, 
                             ties,
                             by = "text")

tweet_sentiment
```

Now that we have these joined, we can take a quick look at the sentiment
.

```{r}
user_sentiment <- tweet_sentiment %>%
  group_by(sender) %>%
  summarise(sentiment = mean(compound))

user_sentiment
```

Note that we have effectively just created some new node and edge
"attributes" that could be incorporated into our network visualization
to potentially help understand why groups may have formed as they did.

### **ðŸ‘‰ Your Turn** **â¤µ**

Recall from the Prepare section that the final question guiding this
analysis was:

> Which actors in our network tend to be more opposed to the Common
> Core?

Use the code chunk below to inspect our data frame and answer the
question above in the space below:

```{r}
#YOUR CODE HERE
```

-   YOUR RESPONSE HERE

------------------------------------------------------------------------

# 5. COMMUNICATE

For your Independent Analysis assignment for Unit 2 next week, you'll
create either a simple report or slide deck using an R Markdown document
just like this to share out some key findings from your analysis.
Regardless of whether you plan to talk us through your analysis and
findings with a presentation or walk us through with a brief written
report, your presentation or report should address the following
questions:

1.  **Purpose**. What question or questions are guiding your analysis?
    What did you hope to learn by answering these questions and why
    should your audience care about your findings?

2.  **Methods**. What data did you selected for analysis? What steps did
    you take took to prepare your data for analysis and what techniques
    you used to analyze your data? These should be fairly explicit with
    your embedded code.

3.  **Findings**. What did you ultimately find? How do your "data
    products" help to illustrate these findings? What conclusions can
    you draw from your analysis?

4.  **Discussion**. What were some of the strengths and weaknesses of
    your analysis? How might your audience use this information? How
    might you revisit or improve upon this analysis in the future?

### **ðŸ‘‰ Your Turn** **â¤µ**

Now that you've become more familiar with this dataset and the social
network perspective, what other aspects of this dataset, or a dataset
you are interested in exploring, could you investigate?

-   YOUR RESPONSE HERE

What specific research questions might you ask that would be helpful for
being understanding and improving learning, or the context in which the
data is collected? How could other approaches like sentiment analysis

-   YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congrats! You've finished the Unit 2 Guided Walkthrough and are ready
for some independent analysis next week!

To complete this assignment, knit your document and send me an email at
sbkellog\@ncsu.edu letting me know you're all set.
